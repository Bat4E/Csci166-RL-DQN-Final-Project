{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bat4E/Csci166-RL-DQN-Final-Project/blob/main/notebooks/DonkeyKongBaseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 3 --- Final Test for 06.11.2025"
      ],
      "metadata": {
        "id": "XtOuIKifmmyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari,accept-rom-license]\n",
        "!pip install autorom\n",
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqL9333gml9l",
        "outputId": "4efa7cb0-bbdb-4951-9cd3-997e74cefb7f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (0.11.2)\n",
            "Collecting autorom\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from autorom) (8.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from autorom) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (2025.10.5)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Installing collected packages: autorom\n",
            "Successfully installed autorom-0.6.1\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (1.2.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.8.0+cu126)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.3)\n",
            "Downloading stable_baselines3-2.7.0-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi2qmVoVmxve",
        "outputId": "fb987184-f824-41a7-8ea0-b974dde76bd1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.12/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/adventure.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/air_raid.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/alien.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/amidar.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/assault.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/asterix.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/asteroids.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/atlantis.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/atlantis2.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/backgammon.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/bank_heist.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/basic_math.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/battle_zone.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/beam_rider.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/berzerk.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/blackjack.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/bowling.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/boxing.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/breakout.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/carnival.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/casino.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/centipede.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/chopper_command.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/combat.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/crazy_climber.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/crossbow.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/darkchambers.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/defender.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/demon_attack.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/donkey_kong.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/double_dunk.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/earthworld.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/elevator_action.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/enduro.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/entombed.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/et.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/fishing_derby.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/flag_capture.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/freeway.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/frogger.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/frostbite.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/galaxian.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/gopher.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/gravitar.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/hangman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/haunted_house.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/hero.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/human_cannonball.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/ice_hockey.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/jamesbond.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/journey_escape.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/joust.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/kaboom.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/kangaroo.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/keystone_kapers.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/king_kong.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/klax.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/koolaid.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/krull.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/kung_fu_master.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/laser_gates.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/lost_luggage.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/mario_bros.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/maze_craze.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/miniature_golf.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/montezuma_revenge.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/mr_do.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/ms_pacman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/name_this_game.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/othello.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pacman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/phoenix.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pitfall.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pitfall2.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pong.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pooyan.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/private_eye.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/qbert.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/riverraid.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/road_runner.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/robotank.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/seaquest.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/sir_lancelot.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/skiing.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/solaris.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/space_invaders.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/space_war.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/star_gunner.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/superman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/surround.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tennis.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tetris.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tic_tac_toe_3d.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/time_pilot.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/trondead.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/turmoil.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tutankham.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/up_n_down.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/venture.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_checkers.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_chess.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_cube.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_pinball.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/warlords.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/wizard_of_wor.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/word_zapper.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/yars_revenge.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/zaxxon.bin\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install the Gym"
      ],
      "metadata": {
        "id": "RQyqgMFzT-qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ale_py\n",
        "import gymnasium as gym"
      ],
      "metadata": {
        "id": "_W_afhrzUAnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure the model save drive"
      ],
      "metadata": {
        "id": "E0n7zQrALq7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v8hSrWrrLibc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e84e10-46bc-4623-fa56-d4f3f077d4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Define directories\n",
        "save_dir_drive = \"/content/drive/MyDrive/PUBLIC/Models\"\n",
        "save_dir_local = \"saved_models\"\n",
        "video_dir_drive = \"/content/drive/MyDrive/PUBLIC/Videos\" # <--- ADDED THIS LINE\n",
        "\n",
        "# Create both directories if they don't exist\n",
        "os.makedirs(save_dir_drive, exist_ok=True)\n",
        "os.makedirs(save_dir_local, exist_ok=True)\n",
        "os.makedirs(video_dir_drive, exist_ok=True) # <--- ADDED THIS LINE"
      ],
      "metadata": {
        "id": "vlWPUjKfLv9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Model"
      ],
      "metadata": {
        "id": "SAEvyoukL1T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import argparse\n",
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "import typing as tt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.tensorboard.writer import SummaryWriter"
      ],
      "metadata": {
        "id": "VCbjkLLSxCUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dqn_model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "    def forward(self, x: torch.ByteTensor):\n",
        "        x = x.float() / 255.0\n",
        "        return self.fc(self.conv(x))"
      ],
      "metadata": {
        "id": "dIJ32Rs6xJsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrappers\n",
        "#wrappers\n",
        "\n",
        "import collections\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import typing as tt\n",
        "\n",
        "from gymnasium import spaces\n",
        "from gymnasium.wrappers import RecordVideo  # added to Record Videos\n",
        "from stable_baselines3.common import atari_wrappers\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    ImageToPyTorch: Reorders image dimensions from (H, W, C) to (C, H, W)\n",
        "    for compatibility with PyTorch convolutional layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        obs = self.observation_space\n",
        "        assert isinstance(obs, gym.spaces.Box)\n",
        "        assert len(obs.shape) == 3\n",
        "        new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1])\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=obs.low.min(), high=obs.high.max(),\n",
        "            shape=new_shape, dtype=obs.dtype)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    BufferWrapper: Maintains a rolling window of the last `n_steps` frames\n",
        "    to give the agent a sense of temporal context.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        obs = env.observation_space\n",
        "        assert isinstance(obs, spaces.Box)\n",
        "        new_obs = gym.spaces.Box(\n",
        "            obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0),\n",
        "            dtype=obs.dtype)\n",
        "        self.observation_space = new_obs\n",
        "        self.buffer = collections.deque(maxlen=n_steps)\n",
        "\n",
        "    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None):\n",
        "        for _ in range(self.buffer.maxlen):\n",
        "            self.buffer.append(np.zeros_like(self.env.observation_space.low))\n",
        "        obs, extra = self.env.reset()\n",
        "        return self.observation(obs), extra\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        self.buffer.append(observation)\n",
        "        return np.concatenate(self.buffer)\n",
        "\n",
        "\n",
        "def make_env(env_name: str, n_steps=4, render_mode=None,\n",
        "             record_video=False, video_folder=None,\n",
        "             episode_trigger=None, video_prefix=None):\n",
        "    print(f\"Creating environment {env_name}\")\n",
        "    env = gym.make(env_name, render_mode=render_mode)\n",
        "    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
        "    if record_video:\n",
        "        env = RecordVideo(\n",
        "            env,\n",
        "            video_folder=video_folder,\n",
        "            episode_trigger=episode_trigger if episode_trigger is not None else lambda x: x == 0,\n",
        "            name_prefix=video_prefix if video_prefix else f\"{env_name.replace('/', '_')}\"\n",
        "        )\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, n_steps=n_steps)\n",
        "    return env\n",
        "\n",
        "\n",
        "# --- Helper function for fixed-length video recording (~10s clips) ---\n",
        "def record_fixed_steps(env_name, video_folder, prefix, steps=600, epsilon=1.0, net=None):\n",
        "    \"\"\"\n",
        "    Record a short clip (~10 seconds) of agent gameplay.\n",
        "    - env_name: Atari env string\n",
        "    - video_folder: where to save video\n",
        "    - prefix: filename prefix\n",
        "    - steps: number of frames (~600 \\u2248 10s at 60fps)\n",
        "    - epsilon: exploration rate (1.0 = random, 0.0 = greedy)\n",
        "    - net: trained network (None = random actions only)\n",
        "    \"\"\"\n",
        "    # Modified: Use make_env to ensure proper preprocessing\n",
        "    env = make_env(\n",
        "        env_name=env_name,\n",
        "        render_mode=\"rgb_array\",\n",
        "        record_video=True,\n",
        "        video_folder=video_folder,\n",
        "        episode_trigger=lambda ep: ep == 0,\n",
        "        video_prefix=prefix\n",
        "    )\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    for step in range(steps):\n",
        "        if random.random() < epsilon or net is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                s = torch.tensor(state).unsqueeze(0).to(torch.float32)\n",
        "                action = net(s).argmax().item()\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        if terminated or truncated:\n",
        "            state, _ = env.reset()\n",
        "    env.close()\n",
        "    print(f\"{prefix} video recorded, reward {total_reward}\")"
      ],
      "metadata": {
        "id": "Vk2CtGcOBcQo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c97b3b-bd33-4691-98de-252b9d8b8aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Base Configuration\n",
        "DEFAULT_ENV_NAME = \"ALE/DonkeyKong-v5\" # Changed to Donkey Kong\n",
        "MEAN_REWARD_BOUND = 500 # Raised from 5 to 500 for meaningful training\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 500\n",
        "REPLAY_START_SIZE = 1000\n",
        "\n",
        "SAVE_EPSILON = 1.0  # Only save if at least this much better\n",
        "EPSILON_DECAY_LAST_FRAME = 10000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.02\n",
        "\n",
        "# Threshold for recording \"learned\" video\n",
        "LEARNED_VIDEO_REWARD_THRESHOLD = 500\n",
        "\n",
        "# Tuple of tensors returned from a sampled minibatch in replay buffer\n",
        "State = np.ndarray\n",
        "Action = int\n",
        "BatchTensors = tt.Tuple[\n",
        "    torch.ByteTensor,           # current state\n",
        "    torch.LongTensor,           # actions\n",
        "    torch.Tensor,               # rewards\n",
        "    torch.BoolTensor,           # done || trunc\n",
        "    torch.ByteTensor            # next state\n",
        "]"
      ],
      "metadata": {
        "id": "GvXPdjPCBxOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚öôÔ∏è Fast Training Config for Quick Test Run\n",
        "MEAN_REWARD_BOUND = 500 # raised to 500\n",
        "REPLAY_START_SIZE = 1000\n",
        "EPSILON_DECAY_LAST_FRAME = 10_000\n",
        "SYNC_TARGET_FRAMES = 500\n",
        "\n",
        "# REPLAY_SIZE = 5000  # optional\n",
        "# BATCH_SIZE = 16     # optional"
      ],
      "metadata": {
        "id": "3FHvHNMrR9Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Define directories\n",
        "save_dir_drive = \"/content/drive/MyDrive/PUBLIC/Models\"\n",
        "save_dir_local = \"saved_models\"\n",
        "\n",
        "# Create both directories if they don't exist\n",
        "os.makedirs(save_dir_drive, exist_ok=True)\n",
        "os.makedirs(save_dir_local, exist_ok=True)\n",
        "\n",
        "# Safe model filename\n",
        "env_name = DEFAULT_ENV_NAME\n",
        "safe_env_name = env_name.replace(\"/\", \"_\")"
      ],
      "metadata": {
        "id": "MkILCuT2OhBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Experience:\n",
        "    state: State\n",
        "    action: Action\n",
        "    reward: float\n",
        "    done_trunc: bool\n",
        "    new_state: State\n",
        "\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience: Experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size: int) -> tt.List[Experience]:\n",
        "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
        "        return [self.buffer[idx] for idx in indices]"
      ],
      "metadata": {
        "id": "uW4Bo-mcB6rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.state: tt.Optional[np.ndarray] = None\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state, _ = self.env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play_step(self, net: DQN, device: torch.device,\n",
        "                  epsilon: float = 0.0) -> tt.Optional[float]:\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            state_v = torch.as_tensor(self.state).to(device)\n",
        "            state_v.unsqueeze_(0)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, is_tr, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(\n",
        "            state=self.state, action=action, reward=float(reward),\n",
        "            done_trunc=is_done or is_tr, new_state=new_state\n",
        "        )\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done or is_tr:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ],
      "metadata": {
        "id": "irJb4V32B-R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
        "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
        "    for e in batch:\n",
        "        states.append(e.state)\n",
        "        actions.append(e.action)\n",
        "        rewards.append(e.reward)\n",
        "        dones.append(e.done_trunc)\n",
        "        new_state.append(e.new_state)\n",
        "    states_t = torch.as_tensor(np.asarray(states))\n",
        "    actions_t = torch.LongTensor(actions)\n",
        "    rewards_t = torch.FloatTensor(rewards)\n",
        "    dones_t = torch.BoolTensor(dones)\n",
        "    new_states_t = torch.as_tensor(np.asarray(new_state))\n",
        "    return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\n",
        "           dones_t.to(device),  new_states_t.to(device)"
      ],
      "metadata": {
        "id": "vHXmNr_wCBJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN,\n",
        "              device: torch.device) -> torch.Tensor:\n",
        "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device)\n",
        "\n",
        "    state_action_values = net(states_t).gather(\n",
        "        1, actions_t.unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "    with torch.no_grad():\n",
        "        next_state_values = tgt_net(new_states_t).max(1)[0]\n",
        "        next_state_values[dones_t] = 0.0\n",
        "        next_state_values = next_state_values.detach()\n",
        "\n",
        "    expected_state_action_values = next_state_values * GAMMA + rewards_t\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ],
      "metadata": {
        "id": "-dbh0431CEXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Training Loop**"
      ],
      "metadata": {
        "id": "lBppsi-ANZYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'model_comment', 'safe_env_name', 'DEFAULT_ENV_NAME',\n",
        "# 'save_dir_drive', 'save_dir_local' are already defined earlier in your script.\n",
        "# Also ensure 'video_dir_drive' is defined and its directory created.\n",
        "# E.g., add this where save_dir_drive and save_dir_local are defined:\n",
        "# video_dir_drive = \"/content/drive/MyDrive/PUBLIC/Videos\"\n",
        "# os.makedirs(video_dir_drive, exist_ok=True)\n",
        "\n",
        "# --- Helper function (patched to use make_env and device) ---\n",
        "def record_fixed_steps(env_name, video_folder, prefix, steps=600, epsilon=1.0, net=None, device=\"cpu\"):\n",
        "    env = make_env(env_name, n_steps=4, render_mode=\"rgb_array\",\n",
        "                   record_video=True, video_folder=video_folder,\n",
        "                   episode_trigger=lambda ep: ep == 0,\n",
        "                   video_prefix=prefix)\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    for step in range(steps):\n",
        "        if random.random() < epsilon or net is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                s = torch.tensor(np.array(state)).unsqueeze(0).to(torch.float32).to(device)\n",
        "                action = net(s).argmax().item()\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        if terminated or truncated:\n",
        "            state, _ = env.reset()\n",
        "    env.close()\n",
        "    print(f\"{prefix} video recorded, reward {total_reward}\")\n",
        "\n",
        "\n",
        "model_comment = f\"test_epsdec{EPSILON_DECAY_LAST_FRAME}_rs{REPLAY_START_SIZE}_sync{SYNC_TARGET_FRAMES}\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Capture Early/Randomish Video (before training starts) ---\n",
        "print(\"\\n--- Recording Early/Randomish Video ---\")\n",
        "record_fixed_steps(DEFAULT_ENV_NAME, video_dir_drive, \"base_start\", steps=600, epsilon=1.0, device=device)\n",
        "\n",
        "# --- Main Training Setup ---\n",
        "env = make_env(env_name=DEFAULT_ENV_NAME)\n",
        "\n",
        "print(f\"\\nObservation space shape: {env.observation_space.shape}\")\n",
        "print(f\"Number of actions: {env.action_space.n}\")\n",
        "\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "writer = SummaryWriter(comment=f\"-{DEFAULT_ENV_NAME.replace('/', '_')}-{model_comment}\")\n",
        "print(net)\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "learned_video_recorded = False\n",
        "LEARNED_VIDEO_REWARD_THRESHOLD = 500\n",
        "\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, device, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        elapsed = time.time() - start_time\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "\n",
        "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "        print(f\"[DEBUG] Episode finished with reward {reward:.2f}, \"\n",
        "              f\"mean_100 {m_reward:.2f}, length {frame_idx - ts_frame} frames\")\n",
        "\n",
        "        if best_m_reward is None or m_reward > best_m_reward + SAVE_EPSILON:\n",
        "            print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "                f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "            model_filename = f\"{safe_env_name}-best_{int(m_reward)}-{timestamp}-{model_comment}.dat\"\n",
        "\n",
        "            model_path_drive = os.path.join(save_dir_drive, model_filename)\n",
        "            model_path_local = os.path.join(save_dir_local, model_filename)\n",
        "\n",
        "            torch.save(net.state_dict(), model_path_drive)\n",
        "            torch.save(net.state_dict(), model_path_local)\n",
        "\n",
        "            print(f\"üíæ Model saved to:\\n - Google Drive: {model_path_drive}\\n - Local:        {model_path_local}\")\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "\n",
        "            # --- End clip when threshold reached ---\n",
        "            if not learned_video_recorded and m_reward > LEARNED_VIDEO_REWARD_THRESHOLD:\n",
        "                print(\"\\n--- Recording Later/Learned Video ---\")\n",
        "                record_fixed_steps(DEFAULT_ENV_NAME, video_dir_drive, \"base_end\", steps=600, epsilon=0.0, net=net, device=device)\n",
        "                learned_video_recorded = True\n",
        "                print(\"Later/Learned video recorded.\")\n",
        "\n",
        "    # --- Midpoint clip ---\n",
        "    if frame_idx == 250000:\n",
        "        print(\"\\n--- Recording Midpoint Video ---\")\n",
        "        record_fixed_steps(DEFAULT_ENV_NAME, video_dir_drive, \"base_mid\", steps=600, epsilon=0.1, net=net, device=device)\n",
        "        print(\"Midpoint video recorded.\")\n",
        "\n",
        "    if frame_idx > 500000 and m_reward > MEAN_REWARD_BOUND:\n",
        "        print(\"Solved in %d frames!\" % frame_idx)\n",
        "        break\n",
        "\n",
        "    if frame_idx % 2000 == 0:\n",
        "        print(f\"[DEBUG] Buffer size: {len(buffer)} / {REPLAY_START_SIZE}\")\n",
        "\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "        print(f\"[DEBUG] Target network synced at frame {frame_idx}\")\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if frame_idx % 1000 == 0:\n",
        "        print(f\"[DEBUG] Loss at frame {frame_idx}: {loss_t.item():.6f}\")\n",
        "\n",
        "    if frame_idx % 10000 == 0:\n",
        "        sample_batch = buffer.sample(BATCH_SIZE)\n",
        "        actions = [exp.action for exp in sample_batch]\n",
        "        unique, counts = np.unique(actions, return_counts=True)\n",
        "        print(f\"[DEBUG] Action distribution at {frame_idx}: {dict(zip(unique, counts))}\")\n",
        "\n",
        "env.close()\n",
        "writer.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ErPlBzGW5fSr",
        "outputId": "b5cbd802-170b-480e-8070-42925fbe7072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Recording Early/Randomish Video ---\n",
            "Creating environment ALE/DonkeyKong-v5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/drive/MyDrive/PUBLIC/Videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_start video recorded, reward 100.0\n",
            "Creating environment ALE/DonkeyKong-v5\n",
            "\n",
            "Observation space shape: (4, 84, 84)\n",
            "Number of actions: 18\n",
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=18, bias=True)\n",
            "  )\n",
            ")\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 0.00, length 0 frames\n",
            "68: done 1 games, reward 0.000, eps 0.99, speed 264.50 f/s, time 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_0-20251108-0626-test_epsdec10000_rs1000_sync500.dat\n",
            " - Local:        saved_models/ALE_DonkeyKong-v5-best_0-20251108-0626-test_epsdec10000_rs1000_sync500.dat\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 0.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 0.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 25.00, length 0 frames\n",
            "377: done 4 games, reward 25.000, eps 0.96, speed 273.18 f/s, time 0.1 min\n",
            "üíæ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_25-20251108-0626-test_epsdec10000_rs1000_sync500.dat\n",
            " - Local:        saved_models/ALE_DonkeyKong-v5-best_25-20251108-0626-test_epsdec10000_rs1000_sync500.dat\n",
            "Best reward updated 0.000 -> 25.000\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 20.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 33.33, length 0 frames\n",
            "515: done 6 games, reward 33.333, eps 0.95, speed 256.32 f/s, time 0.1 min\n",
            "üíæ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_33-20251108-0626-test_epsdec10000_rs1000_sync500.dat\n",
            " - Local:        saved_models/ALE_DonkeyKong-v5-best_33-20251108-0626-test_epsdec10000_rs1000_sync500.dat\n",
            "Best reward updated 25.000 -> 33.333\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 28.57, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 37.50, length 0 frames\n",
            "682: done 8 games, reward 37.500, eps 0.93, speed 256.10 f/s, time 0.1 min\n",
            "üíæ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_37-20251108-0626-test_epsdec10000_rs1000_sync500.dat\n",
            " - Local:        saved_models/ALE_DonkeyKong-v5-best_37-20251108-0626-test_epsdec10000_rs1000_sync500.dat\n",
            "Best reward updated 33.333 -> 37.500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 33.33, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 30.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 27.27, length 0 frames\n",
            "[DEBUG] Target network synced at frame 1000\n",
            "[DEBUG] Loss at frame 1000: 0.002121\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 33.33, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 30.77, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 28.57, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 26.67, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 23.53, length 0 frames\n",
            "[DEBUG] Target network synced at frame 1500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 22.22, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 26.32, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 23.81, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 22.73, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 21.74, length 0 frames\n",
            "[DEBUG] Buffer size: 2000 / 1000\n",
            "[DEBUG] Target network synced at frame 2000\n",
            "[DEBUG] Loss at frame 2000: 2.005219\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 25.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 24.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 23.08, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 25.93, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 28.57, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 27.59, length 0 frames\n",
            "[DEBUG] Target network synced at frame 2500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 26.67, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.81, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 24.24, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 26.47, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.71, length 0 frames\n",
            "[DEBUG] Target network synced at frame 3000\n",
            "[DEBUG] Loss at frame 3000: 1.106555\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 27.78, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 27.03, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 28.95, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 28.21, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 30.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 29.27, length 0 frames\n",
            "[DEBUG] Target network synced at frame 3500\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 30.95, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 32.56, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 31.82, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 31.11, length 0 frames\n",
            "[DEBUG] Buffer size: 4000 / 1000\n",
            "[DEBUG] Target network synced at frame 4000\n",
            "[DEBUG] Loss at frame 4000: 232.561829\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 30.43, length 0 frames\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 34.04, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 33.33, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 32.65, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 34.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 37.25, length 0 frames\n",
            "[DEBUG] Target network synced at frame 4500\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 38.46, length 0 frames\n",
            "4629: done 52 games, reward 38.462, eps 0.54, speed 8.72 f/s, time 6.8 min\n",
            "üíæ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_38-20251108-0633-test_epsdec10000_rs1000_sync500.dat\n",
            " - Local:        saved_models/ALE_DonkeyKong-v5-best_38-20251108-0633-test_epsdec10000_rs1000_sync500.dat\n",
            "Best reward updated 37.500 -> 38.462\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.74, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.04, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 38.18, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 39.29, length 0 frames\n",
            "4954: done 56 games, reward 39.286, eps 0.50, speed 8.86 f/s, time 7.4 min\n",
            "üíæ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_39-20251108-0633-test_epsdec10000_rs1000_sync500.dat\n",
            " - Local:        saved_models/ALE_DonkeyKong-v5-best_39-20251108-0633-test_epsdec10000_rs1000_sync500.dat\n",
            "Best reward updated 38.462 -> 39.286\n",
            "[DEBUG] Target network synced at frame 5000\n",
            "[DEBUG] Loss at frame 5000: 7.770770\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 38.60, length 0 frames\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 41.38, length 0 frames\n",
            "5130: done 58 games, reward 41.379, eps 0.49, speed 9.70 f/s, time 7.7 min\n",
            "üíæ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_41-20251108-0634-test_epsdec10000_rs1000_sync500.dat\n",
            " - Local:        saved_models/ALE_DonkeyKong-v5-best_41-20251108-0634-test_epsdec10000_rs1000_sync500.dat\n",
            "Best reward updated 39.286 -> 41.379\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.68, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 40.98, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.32, length 0 frames\n",
            "[DEBUG] Target network synced at frame 5500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 39.68, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 39.06, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 38.46, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.88, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.31, length 0 frames\n",
            "[DEBUG] Buffer size: 6000 / 1000\n",
            "[DEBUG] Target network synced at frame 6000\n",
            "[DEBUG] Loss at frame 6000: 22.941357\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 36.76, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 37.68, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.14, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 36.62, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 36.11, length 0 frames\n",
            "[DEBUG] Target network synced at frame 6500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 35.62, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 35.14, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 34.67, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 34.21, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 33.77, length 0 frames\n",
            "[DEBUG] Target network synced at frame 7000\n",
            "[DEBUG] Loss at frame 7000: 29.920248\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 33.33, length 0 frames\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 35.44, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 35.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 34.57, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 34.15, length 0 frames\n",
            "[DEBUG] Target network synced at frame 7500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 33.73, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 33.33, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 32.94, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 32.56, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 32.18, length 0 frames\n",
            "[DEBUG] Buffer size: 8000 / 1000\n",
            "[DEBUG] Target network synced at frame 8000\n",
            "[DEBUG] Loss at frame 8000: 62.746338\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 31.82, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 31.46, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 31.11, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 30.77, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 30.43, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 30.11, length 0 frames\n",
            "[DEBUG] Target network synced at frame 8500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 29.79, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 29.47, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 29.17, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 28.87, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 28.57, length 0 frames\n",
            "[DEBUG] Target network synced at frame 9000\n",
            "[DEBUG] Loss at frame 9000: 18.338860\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 28.28, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 28.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 28.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 28.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 28.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 9500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 27.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 27.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 27.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 27.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 26.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 26.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 10000\n",
            "[DEBUG] Loss at frame 10000: 51.121250\n",
            "[DEBUG] Action distribution at 10000: {np.int64(0): np.int64(1), np.int64(1): np.int64(2), np.int64(2): np.int64(3), np.int64(4): np.int64(4), np.int64(5): np.int64(2), np.int64(6): np.int64(1), np.int64(7): np.int64(2), np.int64(8): np.int64(1), np.int64(9): np.int64(2), np.int64(11): np.int64(4), np.int64(12): np.int64(2), np.int64(13): np.int64(3), np.int64(15): np.int64(1), np.int64(16): np.int64(1), np.int64(17): np.int64(3)}\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 26.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 26.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 10500\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 26.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 26.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 26.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 26.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 26.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 11000\n",
            "[DEBUG] Loss at frame 11000: 25.282467\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 24.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 11500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 24.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 24.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 23.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 22.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 22.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 12000\n",
            "[DEBUG] Loss at frame 12000: 39.256859\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 22.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 22.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 22.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 22.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 21.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 21.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 12500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 20.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 20.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 19.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 19.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 18.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 13000\n",
            "[DEBUG] Loss at frame 13000: 71.626503\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 18.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 17.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 16.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 18.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 18.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 13500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 18.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 16.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 16.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 16.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 15.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 13.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 14000\n",
            "[DEBUG] Loss at frame 14000: 179.951614\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 12.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 12.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 12.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 11.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 14500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 15000\n",
            "[DEBUG] Loss at frame 15000: 34.617428\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 15500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 16000\n",
            "[DEBUG] Loss at frame 16000: 35.833473\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 16500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 17000\n",
            "[DEBUG] Loss at frame 17000: 43.108952\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 17500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 18000\n",
            "[DEBUG] Loss at frame 18000: 22.911963\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 18500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 19000\n",
            "[DEBUG] Loss at frame 19000: 27.929857\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 19500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 20000\n",
            "[DEBUG] Loss at frame 20000: 20.990494\n",
            "[DEBUG] Action distribution at 20000: {np.int64(0): np.int64(1), np.int64(1): np.int64(8), np.int64(2): np.int64(2), np.int64(5): np.int64(1), np.int64(9): np.int64(1), np.int64(10): np.int64(1), np.int64(12): np.int64(3), np.int64(13): np.int64(2), np.int64(14): np.int64(3), np.int64(15): np.int64(4), np.int64(16): np.int64(1), np.int64(17): np.int64(5)}\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 20500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 21000\n",
            "[DEBUG] Loss at frame 21000: 32.862038\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 21500\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 22000\n",
            "[DEBUG] Loss at frame 22000: 34.375858\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 22500\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 23000\n",
            "[DEBUG] Loss at frame 23000: 47.857632\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 23500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 24000\n",
            "[DEBUG] Loss at frame 24000: 29.254372\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 24500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 25000\n",
            "[DEBUG] Loss at frame 25000: 35.915482\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 25500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 26000\n",
            "[DEBUG] Loss at frame 26000: 22.073668\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 26500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 27000\n",
            "[DEBUG] Loss at frame 27000: 19.334766\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 27500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 28000\n",
            "[DEBUG] Loss at frame 28000: 17.432766\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 28500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 29000\n",
            "[DEBUG] Loss at frame 29000: 16.121983\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 10.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 29500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 30000\n",
            "[DEBUG] Loss at frame 30000: 20.733469\n",
            "[DEBUG] Action distribution at 30000: {np.int64(1): np.int64(5), np.int64(2): np.int64(3), np.int64(3): np.int64(2), np.int64(5): np.int64(1), np.int64(6): np.int64(1), np.int64(7): np.int64(3), np.int64(9): np.int64(1), np.int64(10): np.int64(2), np.int64(12): np.int64(2), np.int64(13): np.int64(2), np.int64(15): np.int64(6), np.int64(16): np.int64(3), np.int64(17): np.int64(1)}\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 30500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 31000\n",
            "[DEBUG] Loss at frame 31000: 51.160084\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 9.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 31500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 32000\n",
            "[DEBUG] Loss at frame 32000: 29.853947\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 32500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 8.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 33000\n",
            "[DEBUG] Loss at frame 33000: 21.606953\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 33500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 7.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 34000\n",
            "[DEBUG] Loss at frame 34000: 40.230629\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 34500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 35000\n",
            "[DEBUG] Loss at frame 35000: 58.261692\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 35500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 36000\n",
            "[DEBUG] Loss at frame 36000: 8.683661\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 36500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 37000\n",
            "[DEBUG] Loss at frame 37000: 26.756859\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 37500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 38000\n",
            "[DEBUG] Loss at frame 38000: 6.453991\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 38500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 39000\n",
            "[DEBUG] Loss at frame 39000: 13.022141\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 5.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 39500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 6.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Buffer size: 10000 / 1000\n",
            "[DEBUG] Target network synced at frame 40000\n",
            "[DEBUG] Loss at frame 40000: 14.455229\n",
            "[DEBUG] Action distribution at 40000: {np.int64(1): np.int64(2), np.int64(2): np.int64(2), np.int64(4): np.int64(1), np.int64(6): np.int64(1), np.int64(7): np.int64(1), np.int64(8): np.int64(1), np.int64(9): np.int64(4), np.int64(12): np.int64(6), np.int64(13): np.int64(1), np.int64(15): np.int64(5), np.int64(16): np.int64(2), np.int64(17): np.int64(6)}\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Target network synced at frame 40500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 4.00, length 0 frames\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2862748394.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}