{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bat4E/Csci166-RL-DQN-Final-Project/blob/main/notebooks/Variant_Donkey_Kong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 3 --- Final Test for 06.11.2025"
      ],
      "metadata": {
        "id": "XtOuIKifmmyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari,accept-rom-license]\n",
        "!pip install autorom\n",
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqL9333gml9l",
        "outputId": "308135d6-6845-46d1-c90d-9f760d17993c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (0.11.2)\n",
            "Requirement already satisfied: autorom in /usr/local/lib/python3.12/dist-packages (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from autorom) (8.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from autorom) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (2025.10.5)\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (1.2.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.8.0+cu126)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi2qmVoVmxve",
        "outputId": "0808538d-4bce-4454-8954-660f83eb5b43",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.12/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install the Gym"
      ],
      "metadata": {
        "id": "RQyqgMFzT-qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ale_py\n",
        "import gymnasium as gym"
      ],
      "metadata": {
        "id": "_W_afhrzUAnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure the model save drive"
      ],
      "metadata": {
        "id": "E0n7zQrALq7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v8hSrWrrLibc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd4c4dd-33bb-4147-f82e-17260d915d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Define directories\n",
        "save_dir_drive = \"/content/drive/MyDrive/PUBLIC/Models\"\n",
        "save_dir_local = \"saved_models\"\n",
        "video_dir_drive = \"/content/drive/MyDrive/PUBLIC/Videos\" # <--- ADDED THIS LINE\n",
        "\n",
        "# Create both directories if they don't exist\n",
        "os.makedirs(save_dir_drive, exist_ok=True)\n",
        "os.makedirs(save_dir_local, exist_ok=True)\n",
        "os.makedirs(video_dir_drive, exist_ok=True) # <--- ADDED THIS LINE"
      ],
      "metadata": {
        "id": "vlWPUjKfLv9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Model"
      ],
      "metadata": {
        "id": "SAEvyoukL1T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import argparse\n",
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "import typing as tt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.tensorboard.writer import SummaryWriter"
      ],
      "metadata": {
        "id": "VCbjkLLSxCUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DuelingDQN_model\n",
        "# --- Dueling DQN Network ---\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "\n",
        "        self.fc_adv = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512), nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "        self.fc_val = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512), nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = torch.zeros(1, *shape)\n",
        "        return int(np.prod(self.conv(o).size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        adv = self.fc_adv(x)\n",
        "        val = self.fc_val(x)\n",
        "        return val + adv - adv.mean(dim=1, keepdim=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "dIJ32Rs6xJsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrappers\n",
        "#wrappers\n",
        "\n",
        "import collections\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import typing as tt\n",
        "\n",
        "from gymnasium import spaces\n",
        "from gymnasium.wrappers import RecordVideo  # added to Record Videos\n",
        "from stable_baselines3.common import atari_wrappers\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    ImageToPyTorch: Reorders image dimensions from (H, W, C) to (C, H, W)\n",
        "    for compatibility with PyTorch convolutional layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        obs = self.observation_space\n",
        "        assert isinstance(obs, gym.spaces.Box)\n",
        "        assert len(obs.shape) == 3\n",
        "        new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1])\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=obs.low.min(), high=obs.high.max(),\n",
        "            shape=new_shape, dtype=obs.dtype)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    BufferWrapper: Maintains a rolling window of the last `n_steps` frames\n",
        "    to give the agent a sense of temporal context.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        obs = env.observation_space\n",
        "        assert isinstance(obs, spaces.Box)\n",
        "        new_obs = gym.spaces.Box(\n",
        "            obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0),\n",
        "            dtype=obs.dtype)\n",
        "        self.observation_space = new_obs\n",
        "        self.buffer = collections.deque(maxlen=n_steps)\n",
        "\n",
        "    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None):\n",
        "        for _ in range(self.buffer.maxlen):\n",
        "            self.buffer.append(np.zeros_like(self.env.observation_space.low))\n",
        "        obs, extra = self.env.reset()\n",
        "        return self.observation(obs), extra\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        self.buffer.append(observation)\n",
        "        return np.concatenate(self.buffer)\n",
        "\n",
        "\n",
        "def make_env(env_name: str, n_steps=4, render_mode=None,\n",
        "             record_video=False, video_folder=None,\n",
        "             episode_trigger=None, video_prefix=None):\n",
        "    print(f\"Creating environment {env_name}\")\n",
        "    env = gym.make(env_name, render_mode=render_mode)\n",
        "    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
        "    if record_video:\n",
        "        env = RecordVideo(\n",
        "            env,\n",
        "            video_folder=video_folder,\n",
        "            episode_trigger=episode_trigger if episode_trigger is not None else lambda x: x == 0,\n",
        "            name_prefix=video_prefix if video_prefix else f\"{env_name.replace('/', '_')}\"\n",
        "        )\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, n_steps=n_steps)\n",
        "    return env\n",
        "\n",
        "\n",
        "# --- Helper function for fixed-length video recording (~10s clips) ---\n",
        "def record_fixed_steps(env_name, video_folder, prefix, steps=600, epsilon=1.0, net=None):\n",
        "    \"\"\"\n",
        "    Record a short clip (~10 seconds) of agent gameplay.\n",
        "    - env_name: Atari env string\n",
        "    - video_folder: where to save video\n",
        "    - prefix: filename prefix\n",
        "    - steps: number of frames (~600 \\u2248 10s at 60fps)\n",
        "    - epsilon: exploration rate (1.0 = random, 0.0 = greedy)\n",
        "    - net: trained network (None = random actions only)\n",
        "    \"\"\"\n",
        "    # Modified: Use make_env to ensure proper preprocessing\n",
        "    env = make_env(\n",
        "        env_name=env_name,\n",
        "        render_mode=\"rgb_array\",\n",
        "        record_video=True,\n",
        "        video_folder=video_folder,\n",
        "        episode_trigger=lambda ep: ep == 0,\n",
        "        video_prefix=prefix\n",
        "    )\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    for step in range(steps):\n",
        "        if random.random() < epsilon or net is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                s = torch.tensor(state).unsqueeze(0).to(torch.float32)\n",
        "                action = net(s).argmax().item()\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        if terminated or truncated:\n",
        "            state, _ = env.reset()\n",
        "    env.close()\n",
        "    print(f\"{prefix} video recorded, reward {total_reward}\")"
      ],
      "metadata": {
        "id": "Vk2CtGcOBcQo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646d30b8-5f31-46d2-e1c7-41701a667bac",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Base Configuration\n",
        "DEFAULT_ENV_NAME = \"ALE/DonkeyKong-v5\" # Changed to Donkey Kong\n",
        "MEAN_REWARD_BOUND = 500 # Raised from 5 to 500 for meaningful training\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 16\n",
        "REPLAY_SIZE = 5000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 500\n",
        "REPLAY_START_SIZE = 1000\n",
        "\n",
        "SAVE_EPSILON = 1.0  # Only save if at least this much better\n",
        "EPSILON_DECAY_LAST_FRAME = 10000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.02\n",
        "\n",
        "# Threshold for recording \"learned\" video\n",
        "LEARNED_VIDEO_REWARD_THRESHOLD = 500\n",
        "\n",
        "# Tuple of tensors returned from a sampled minibatch in replay buffer\n",
        "State = np.ndarray\n",
        "Action = int\n",
        "BatchTensors = tt.Tuple[\n",
        "    torch.ByteTensor,           # current state\n",
        "    torch.LongTensor,           # actions\n",
        "    torch.Tensor,               # rewards\n",
        "    torch.BoolTensor,           # done || trunc\n",
        "    torch.ByteTensor            # next state\n",
        "]"
      ],
      "metadata": {
        "id": "GvXPdjPCBxOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âš™ï¸ Fast Training Config for Quick Test Run\n",
        "MEAN_REWARD_BOUND = 500 # raised to 500\n",
        "REPLAY_START_SIZE = 1000\n",
        "EPSILON_DECAY_LAST_FRAME = 10_000\n",
        "SYNC_TARGET_FRAMES = 500\n",
        "\n",
        "REPLAY_SIZE = 5000  # optional, uncommented\n",
        "BATCH_SIZE = 16     # optional, uncommented"
      ],
      "metadata": {
        "id": "3FHvHNMrR9Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Define directories\n",
        "save_dir_drive = \"/content/drive/MyDrive/PUBLIC/Models\"\n",
        "save_dir_local = \"saved_models\"\n",
        "\n",
        "# Create both directories if they don't exist\n",
        "os.makedirs(save_dir_drive, exist_ok=True)\n",
        "os.makedirs(save_dir_local, exist_ok=True)\n",
        "\n",
        "# Safe model filename\n",
        "env_name = DEFAULT_ENV_NAME\n",
        "safe_env_name = env_name.replace(\"/\", \"_\")"
      ],
      "metadata": {
        "id": "MkILCuT2OhBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prioritized Replay Buffer ---\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity, alpha=0.6):\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.buffer = []\n",
        "        self.pos = 0\n",
        "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
        "\n",
        "    def push(self, experience):\n",
        "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(experience)\n",
        "        else:\n",
        "            self.buffer[self.pos] = experience\n",
        "        self.priorities[self.pos] = max_prio\n",
        "        self.pos = (self.pos + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        if len(self.buffer) == self.capacity:\n",
        "            prios = self.priorities\n",
        "        else:\n",
        "            prios = self.priorities[:self.pos]\n",
        "        probs = prios ** self.alpha\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        idxs = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "        samples = [self.buffer[idx] for idx in idxs]\n",
        "\n",
        "        total = len(self.buffer)\n",
        "        weights = (total * probs[idxs]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "        return [(*samples[i], weights[i], idxs[i]) for i in range(batch_size)]\n",
        "\n",
        "    def update_priorities(self, idxs, prios):\n",
        "        for idx, prio in zip(idxs, prios):\n",
        "            self.priorities[idx] = prio + 1e-5\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "zzeAVRzWGj11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.state = None\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state, _ = self.env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play_step(self, net, device, epsilon: float = 0.0):\n",
        "        done_reward = None\n",
        "\n",
        "        # Îµ-greedy action selection\n",
        "        if np.random.random() < epsilon:\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            # Cast to float32 and normalize to [0,1]\n",
        "            state_v = torch.as_tensor(self.state, dtype=torch.float32).to(device).unsqueeze(0)\n",
        "            state_v /= 255.0\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # Step in environment\n",
        "        new_state, reward, is_done, is_tr, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        # Store experience in PER buffer\n",
        "        exp = (self.state, action, float(reward), is_done or is_tr, new_state)\n",
        "        self.exp_buffer.push(exp)\n",
        "\n",
        "        self.state = new_state\n",
        "        if is_done or is_tr:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward\n"
      ],
      "metadata": {
        "id": "irJb4V32B-R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Double DQN Loss ---\n",
        "def calc_loss(batch, net, tgt_net, device, gamma=GAMMA):\n",
        "    states, actions, rewards, dones, next_states, weights, idxs = zip(*batch)\n",
        "\n",
        "    states_v = torch.tensor(np.array(states), dtype=torch.float32).to(device) / 255.0\n",
        "    actions_v = torch.tensor(actions).to(torch.int64).unsqueeze(-1).to(device)\n",
        "    rewards_v = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "    dones_t = torch.BoolTensor(dones).to(device)\n",
        "    next_states_v = torch.tensor(np.array(next_states), dtype=torch.float32).to(device) / 255.0\n",
        "    weights_v = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "\n",
        "    state_action_values = net(states_v).gather(1, actions_v).squeeze(-1)\n",
        "\n",
        "    # Double DQN\n",
        "    next_state_actions = net(next_states_v).argmax(dim=1, keepdim=True)\n",
        "    next_state_values = tgt_net(next_states_v).gather(1, next_state_actions).squeeze(-1)\n",
        "    next_state_values[dones_t] = 0.0\n",
        "\n",
        "    expected_values = rewards_v + gamma * next_state_values\n",
        "    loss = (state_action_values - expected_values.detach()).pow(2) * weights_v\n",
        "\n",
        "    # detach before numpy\n",
        "    td_errors = (state_action_values.detach() - expected_values.detach()).abs().cpu().numpy()\n",
        "\n",
        "    return loss.mean(), td_errors, idxs\n"
      ],
      "metadata": {
        "id": "-dbh0431CEXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Training Loop**"
      ],
      "metadata": {
        "id": "lBppsi-ANZYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'model_comment', 'safe_env_name', 'DEFAULT_ENV_NAME',\n",
        "# 'save_dir_drive', 'save_dir_local' are already defined earlier in your script.\n",
        "# Also ensure 'video_dir_drive' is defined and its directory created.\n",
        "# E.g., add this where save_dir_drive and save_dir_local are defined:\n",
        "# video_dir_drive = \"/content/drive/MyDrive/PUBLIC/Videos\"\n",
        "# os.makedirs(video_dir_drive, exist_ok=True)\n",
        "\n",
        "# --- Longer Video Recording (~30s = 1800 frames) ---\n",
        "def record_fixed_steps(env_name, video_folder, prefix, steps=1800, epsilon=1.0, net=None, device=\"cpu\"):\n",
        "    env = make_env(env_name, n_steps=4, render_mode=\"rgb_array\",\n",
        "                   record_video=True, video_folder=video_folder,\n",
        "                   episode_trigger=lambda ep: ep == 0,\n",
        "                   video_prefix=prefix)\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    for step in range(steps):\n",
        "        if random.random() < epsilon or net is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                s = torch.tensor(np.array(state)).unsqueeze(0).to(torch.float32).to(device)\n",
        "                action = net(s).argmax().item()\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        if terminated or truncated:\n",
        "            state, _ = env.reset()\n",
        "    env.close()\n",
        "    print(f\"{prefix} video recorded, reward {total_reward}\")\n",
        "\n",
        "\n",
        "# --- Training Loop (Variant Agent) ---\n",
        "model_comment = \"variant_double_dueling_per\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"\\n--- Recording Early/Randomish Video (Variant) ---\")\n",
        "record_fixed_steps(DEFAULT_ENV_NAME, video_dir_drive, \"variant_start\", steps=1800, epsilon=1.0, device=device)\n",
        "\n",
        "env = make_env(env_name=DEFAULT_ENV_NAME)\n",
        "print(f\"\\nObservation space shape: {env.observation_space.shape}\")\n",
        "print(f\"Number of actions: {env.action_space.n}\")\n",
        "\n",
        "net = DuelingDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net = DuelingDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "writer = SummaryWriter(comment=f\"-{DEFAULT_ENV_NAME.replace('/', '_')}-{model_comment}\")\n",
        "print(net)\n",
        "\n",
        "buffer = PrioritizedReplayBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "learned_video_recorded = False\n",
        "LEARNED_VIDEO_REWARD_THRESHOLD = 500\n",
        "\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, device, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        elapsed = time.time() - start_time\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "\n",
        "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "        print(f\"[DEBUG] Episode finished with reward {reward:.2f}, mean_100 {m_reward:.2f}\")\n",
        "\n",
        "        if best_m_reward is None or m_reward > best_m_reward + SAVE_EPSILON:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "            model_filename = f\"{safe_env_name}-best_{int(m_reward)}-{timestamp}-{model_comment}.dat\"\n",
        "            model_path_drive = os.path.join(save_dir_drive, model_filename)\n",
        "            model_path_local = os.path.join(save_dir_local, model_filename)\n",
        "            torch.save(net.state_dict(), model_path_drive)\n",
        "            torch.save(net.state_dict(), model_path_local)\n",
        "            print(f\"ðŸ’¾ Model saved to:\\n - Google Drive: {model_path_drive}\\n - Local: {model_path_local}\")\n",
        "            best_m_reward = m_reward\n",
        "\n",
        "            if not learned_video_recorded and m_reward > LEARNED_VIDEO_REWARD_THRESHOLD:\n",
        "                print(\"\\n--- Recording Later/Learned Video (Variant) ---\")\n",
        "                record_fixed_steps(DEFAULT_ENV_NAME, video_dir_drive, \"variant_end\", steps=1800, epsilon=0.0, net=net, device=device)\n",
        "                learned_video_recorded = True\n",
        "                print(\"Later/Learned video recorded.\")\n",
        "\n",
        "    # --- Midpoint clip ---\n",
        "    if frame_idx == 250000:\n",
        "        print(\"\\n--- Recording Midpoint Video (Variant) ---\")\n",
        "        record_fixed_steps(DEFAULT_ENV_NAME, video_dir_drive, \"variant_mid\", steps=1800, epsilon=0.1, net=net, device=device)\n",
        "        print(\"Midpoint video recorded.\")\n",
        "\n",
        "    if frame_idx > 500000 and m_reward > MEAN_REWARD_BOUND:\n",
        "        print(\"Solved in %d frames!\" % frame_idx)\n",
        "        break\n",
        "\n",
        "    if frame_idx % 2000 == 0:\n",
        "        print(f\"[DEBUG] Buffer size: {len(buffer)} / {REPLAY_START_SIZE}\")\n",
        "\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "        print(f\"[DEBUG] Target network synced at frame {frame_idx}\")\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss_t, prios, idxs = calc_loss(batch, net, tgt_net, device)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update PER priorities\n",
        "    buffer.update_priorities(idxs, prios)\n",
        "\n",
        "    if frame_idx % 1000 == 0:\n",
        "        print(f\"[DEBUG] Loss at frame {frame_idx}: {loss_t.item():.6f}\")\n",
        "\n",
        "    if frame_idx % 10000 == 0:\n",
        "        sample_batch = buffer.sample(BATCH_SIZE)\n",
        "        actions = [exp[1] for exp in sample_batch]  # action is 2nd element\n",
        "        unique, counts = np.unique(actions, return_counts=True)\n",
        "        print(f\"[DEBUG] Action distribution at {frame_idx}: {dict(zip(unique, counts))}\")\n",
        "\n",
        "env.close()\n",
        "writer.close()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ErPlBzGW5fSr",
        "outputId": "b819b4c8-ab71-4ffd-b091-7e200270a7da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Recording Early/Randomish Video (Variant) ---\n",
            "Creating environment ALE/DonkeyKong-v5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/drive/MyDrive/PUBLIC/Videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "variant_start video recorded, reward 1300.0\n",
            "Creating environment ALE/DonkeyKong-v5\n",
            "\n",
            "Observation space shape: (4, 84, 84)\n",
            "Number of actions: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DuelingDQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc_adv): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=18, bias=True)\n",
            "  )\n",
            "  (fc_val): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Episode finished with reward 0.00, mean_100 0.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_0-20251108-2110-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_0-20251108-2110-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 0.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 0.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 0.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 0.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 0.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 0.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 0.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 11.11\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_11-20251108-2110-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_11-20251108-2110-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 20.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_20-20251108-2110-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_20-20251108-2110-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 18.18\n",
            "[DEBUG] Target network synced at frame 1000\n",
            "[DEBUG] Loss at frame 1000: 0.001547\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 16.67\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 15.38\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 14.29\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 20.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 18.75\n",
            "[DEBUG] Target network synced at frame 1500\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 29.41\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_29-20251108-2110-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_29-20251108-2110-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 27.78\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 26.32\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 30.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 28.57\n",
            "[DEBUG] Buffer size: 2000 / 1000\n",
            "[DEBUG] Target network synced at frame 2000\n",
            "[DEBUG] Loss at frame 2000: 73.400887\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 27.27\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 26.09\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 24.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 26.92\n",
            "[DEBUG] Target network synced at frame 2500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.93\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 25.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 24.14\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 23.33\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 22.58\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 25.00\n",
            "[DEBUG] Target network synced at frame 3000\n",
            "[DEBUG] Loss at frame 3000: 27.587706\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 24.24\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 23.53\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 22.86\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 25.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 27.03\n",
            "[DEBUG] Target network synced at frame 3500\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 28.95\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 28.21\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 30.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 29.27\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 30.95\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_30-20251108-2110-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_30-20251108-2110-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 32.56\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_32-20251108-2110-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_32-20251108-2110-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 4000 / 1000\n",
            "[DEBUG] Target network synced at frame 4000\n",
            "[DEBUG] Loss at frame 4000: 98.402985\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 31.82\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 31.11\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 30.43\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 29.79\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 33.33\n",
            "[DEBUG] Target network synced at frame 4500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 32.65\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 34.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_34-20251108-2111-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_34-20251108-2111-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 33.33\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 34.62\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 35.85\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_35-20251108-2111-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_35-20251108-2111-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 5000\n",
            "[DEBUG] Loss at frame 5000: 44.487778\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 35.19\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 34.55\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 35.71\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 35.09\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 36.21\n",
            "[DEBUG] Target network synced at frame 5500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 35.59\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 36.67\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 36.07\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 38.71\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_38-20251108-2111-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_38-20251108-2111-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 38.10\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 6000\n",
            "[DEBUG] Loss at frame 6000: 59.954681\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 42.19\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_42-20251108-2111-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_42-20251108-2111-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 43.08\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 42.42\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.79\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.18\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 42.03\n",
            "[DEBUG] Target network synced at frame 6500\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 42.86\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 42.25\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.67\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.10\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.54\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 41.33\n",
            "[DEBUG] Target network synced at frame 7000\n",
            "[DEBUG] Loss at frame 7000: 34.072430\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.79\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 41.56\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 42.31\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.77\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 42.50\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.98\n",
            "[DEBUG] Target network synced at frame 7500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.46\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 42.17\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.67\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.18\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.70\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 8000\n",
            "[DEBUG] Loss at frame 8000: 102.908630\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.23\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 39.77\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 39.33\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 38.89\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 38.46\n",
            "[DEBUG] Target network synced at frame 8500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 38.04\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.63\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.23\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 36.84\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 36.46\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 36.08\n",
            "[DEBUG] Target network synced at frame 9000\n",
            "[DEBUG] Loss at frame 9000: 84.616348\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 35.71\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 35.35\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.00\n",
            "[DEBUG] Target network synced at frame 9500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 38.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 38.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 38.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 10000\n",
            "[DEBUG] Loss at frame 10000: 25.356064\n",
            "[DEBUG] Action distribution at 10000: {np.int64(7): np.int64(7), np.int64(9): np.int64(2), np.int64(11): np.int64(1), np.int64(13): np.int64(2), np.int64(15): np.int64(1), np.int64(17): np.int64(3)}\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 36.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 36.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 38.00\n",
            "[DEBUG] Target network synced at frame 10500\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 38.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 39.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.00\n",
            "[DEBUG] Target network synced at frame 11000\n",
            "[DEBUG] Loss at frame 11000: 35.413322\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 36.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 37.00\n",
            "[DEBUG] Target network synced at frame 11500\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 37.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 38.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 38.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 39.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 40.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 12000\n",
            "[DEBUG] Loss at frame 12000: 93.150063\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 40.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.00\n",
            "[DEBUG] Target network synced at frame 12500\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 40.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 40.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.00\n",
            "[DEBUG] Target network synced at frame 13000\n",
            "[DEBUG] Loss at frame 13000: 60.512962\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 39.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 40.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.00\n",
            "[DEBUG] Target network synced at frame 13500\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 40.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 40.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 14000\n",
            "[DEBUG] Loss at frame 14000: 33.595863\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 39.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 40.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 41.00\n",
            "[DEBUG] Target network synced at frame 14500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 41.00\n",
            "[DEBUG] Target network synced at frame 15000\n",
            "[DEBUG] Loss at frame 15000: 27.258114\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 39.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 38.00\n",
            "[DEBUG] Target network synced at frame 15500\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 40.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 42.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 42.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 16000\n",
            "[DEBUG] Loss at frame 16000: 58.834419\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 40.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 41.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 42.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 42.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 42.00\n",
            "[DEBUG] Target network synced at frame 16500\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 42.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 43.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 45.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_45-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_45-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 46.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 48.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_48-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_48-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 17000\n",
            "[DEBUG] Loss at frame 17000: 132.661926\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 50.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_50-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_50-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 51.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 52.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_52-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_52-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 53.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 55.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_55-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_55-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 17500\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 56.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 58.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_58-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_58-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 58.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 59.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 59.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 18000\n",
            "[DEBUG] Loss at frame 18000: 126.301361\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 61.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_61-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_61-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 63.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_63-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_63-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 65.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_65-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_65-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 67.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_67-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_67-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 70.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_70-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_70-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 18500\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 73.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_73-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_73-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 76.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_76-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_76-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 78.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_78-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_78-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 78.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 79.00\n",
            "[DEBUG] Target network synced at frame 19000\n",
            "[DEBUG] Loss at frame 19000: 64.531967\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 80.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_80-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_80-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 83.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_83-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_83-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 85.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_85-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_85-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 90.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_90-20251108-2113-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_90-20251108-2113-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 19500\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 91.00\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 97.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_97-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_97-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 99.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_99-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_99-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 20000\n",
            "[DEBUG] Loss at frame 20000: 35.207695\n",
            "[DEBUG] Action distribution at 20000: {np.int64(4): np.int64(5), np.int64(7): np.int64(3), np.int64(9): np.int64(1), np.int64(10): np.int64(1), np.int64(11): np.int64(1), np.int64(12): np.int64(2), np.int64(16): np.int64(1), np.int64(17): np.int64(2)}\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 106.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_106-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_106-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 108.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_108-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_108-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 112.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_112-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_112-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 113.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 115.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_115-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_115-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 20500\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 118.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_118-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_118-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 121.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_121-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_121-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 122.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 123.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_123-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_123-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 21000\n",
            "[DEBUG] Loss at frame 21000: 79.266541\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 126.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_126-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_126-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 127.00\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 130.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_130-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_130-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 130.00\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 137.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_137-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_137-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 21500\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 142.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_142-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_142-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 147.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_147-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_147-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 148.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 22000\n",
            "[DEBUG] Loss at frame 22000: 62.890434\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 158.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_158-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_158-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 158.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 162.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_162-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_162-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 167.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_167-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_167-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 22500\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 168.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 172.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_172-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_172-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 175.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_175-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_175-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 176.00\n",
            "[DEBUG] Target network synced at frame 23000\n",
            "[DEBUG] Loss at frame 23000: 68.742874\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 180.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_180-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_180-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 180.00\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 197.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_197-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_197-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 23500\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 202.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_202-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_202-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 203.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 203.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 203.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 205.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_205-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_205-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 24000\n",
            "[DEBUG] Loss at frame 24000: 196.201675\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 206.00\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 220.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_220-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_220-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 221.00\n",
            "[DEBUG] Target network synced at frame 24500\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 224.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_224-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_224-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 226.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_226-20251108-2114-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_226-20251108-2114-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 900.00, mean_100 235.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_235-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_235-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 25000\n",
            "[DEBUG] Loss at frame 25000: 123.504425\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 254.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_254-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_254-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 25500\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 259.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_259-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_259-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 1600.00, mean_100 275.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_275-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_275-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 26000\n",
            "[DEBUG] Loss at frame 26000: 221.559418\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 292.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_292-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_292-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 294.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_294-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_294-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 26500\n",
            "[DEBUG] Episode finished with reward 1600.00, mean_100 310.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_310-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_310-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 27000\n",
            "[DEBUG] Loss at frame 27000: 61.425278\n",
            "[DEBUG] Episode finished with reward 1600.00, mean_100 325.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_325-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_325-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 900.00, mean_100 334.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_334-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_334-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 333.00\n",
            "[DEBUG] Target network synced at frame 27500\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 343.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_343-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_343-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 28000\n",
            "[DEBUG] Loss at frame 28000: 68.117569\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 359.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_359-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_359-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 365.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_365-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_365-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 365.00\n",
            "[DEBUG] Target network synced at frame 28500\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 377.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_377-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_377-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 29000\n",
            "[DEBUG] Loss at frame 29000: 33.873672\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 397.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_397-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_397-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 29500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 417.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_417-20251108-2115-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_417-20251108-2115-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 418.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 30000\n",
            "[DEBUG] Loss at frame 30000: 102.676109\n",
            "[DEBUG] Action distribution at 30000: {np.int64(1): np.int64(1), np.int64(2): np.int64(1), np.int64(4): np.int64(4), np.int64(7): np.int64(1), np.int64(9): np.int64(1), np.int64(11): np.int64(1), np.int64(12): np.int64(2), np.int64(13): np.int64(2), np.int64(16): np.int64(2), np.int64(17): np.int64(1)}\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 438.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_438-20251108-2116-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_438-20251108-2116-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 30500\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 458.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_458-20251108-2116-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_458-20251108-2116-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 458.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 460.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_460-20251108-2116-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_460-20251108-2116-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 464.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_464-20251108-2116-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_464-20251108-2116-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 31000\n",
            "[DEBUG] Loss at frame 31000: 202.187134\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 474.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_474-20251108-2116-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_474-20251108-2116-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 479.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_479-20251108-2116-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_479-20251108-2116-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 489.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_489-20251108-2116-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_489-20251108-2116-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 31500\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 490.00\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 509.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_509-20251108-2116-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_509-20251108-2116-variant_double_dueling_per.dat\n",
            "\n",
            "--- Recording Later/Learned Video (Variant) ---\n",
            "Creating environment ALE/DonkeyKong-v5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/drive/MyDrive/PUBLIC/Videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "variant_end video recorded, reward 8200.0\n",
            "Later/Learned video recorded.\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 32000\n",
            "[DEBUG] Loss at frame 32000: 158.425873\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 516.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_516-20251108-2116-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_516-20251108-2116-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 32500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 536.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_536-20251108-2116-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_536-20251108-2116-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 542.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_542-20251108-2116-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_542-20251108-2116-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 33000\n",
            "[DEBUG] Loss at frame 33000: 117.049088\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 562.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_562-20251108-2116-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_562-20251108-2116-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 33500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 580.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_580-20251108-2116-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_580-20251108-2116-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 580.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 34000\n",
            "[DEBUG] Loss at frame 34000: 144.661575\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 593.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_593-20251108-2116-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_593-20251108-2116-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 592.00\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 601.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_601-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_601-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 34500\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 604.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_604-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_604-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 605.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 605.00\n",
            "[DEBUG] Target network synced at frame 35000\n",
            "[DEBUG] Loss at frame 35000: 632.588501\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 624.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_624-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_624-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 625.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 627.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_627-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_627-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 35500\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 633.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_633-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_633-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 648.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_648-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_648-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 647.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 36000\n",
            "[DEBUG] Loss at frame 36000: 279.737640\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 657.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_657-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_657-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 659.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_659-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_659-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 36500\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 674.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_674-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_674-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 37000\n",
            "[DEBUG] Loss at frame 37000: 357.481232\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 691.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_691-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_691-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 37500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 708.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_708-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_708-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 720.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_720-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_720-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 38000\n",
            "[DEBUG] Loss at frame 38000: 124.977448\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 729.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_729-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_729-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 739.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_739-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_739-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 738.00\n",
            "[DEBUG] Target network synced at frame 38500\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 751.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_751-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_751-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 39000\n",
            "[DEBUG] Loss at frame 39000: 212.114395\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 768.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_768-20251108-2117-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_768-20251108-2117-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 39500\n",
            "[DEBUG] Episode finished with reward 2300.00, mean_100 789.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_789-20251108-2118-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_789-20251108-2118-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 40000\n",
            "[DEBUG] Loss at frame 40000: 160.049713\n",
            "[DEBUG] Action distribution at 40000: {np.int64(1): np.int64(3), np.int64(2): np.int64(1), np.int64(4): np.int64(3), np.int64(9): np.int64(1), np.int64(12): np.int64(2), np.int64(13): np.int64(3), np.int64(16): np.int64(3)}\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 804.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_804-20251108-2118-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_804-20251108-2118-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 40500\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 824.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_824-20251108-2118-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_824-20251108-2118-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 826.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_826-20251108-2118-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_826-20251108-2118-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 41000\n",
            "[DEBUG] Loss at frame 41000: 284.066345\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 846.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_846-20251108-2118-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_846-20251108-2118-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 840.00\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 846.00\n",
            "[DEBUG] Target network synced at frame 41500\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 844.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 42000\n",
            "[DEBUG] Loss at frame 42000: 611.243958\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 864.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_864-20251108-2118-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_864-20251108-2118-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 900.00, mean_100 871.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_871-20251108-2118-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_871-20251108-2118-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 42500\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 881.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_881-20251108-2118-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_881-20251108-2118-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 880.00\n",
            "[DEBUG] Target network synced at frame 43000\n",
            "[DEBUG] Loss at frame 43000: 198.617218\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 899.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_899-20251108-2118-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_899-20251108-2118-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 912.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_912-20251108-2118-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_912-20251108-2118-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 43500\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 931.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_931-20251108-2118-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_931-20251108-2118-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 44000\n",
            "[DEBUG] Loss at frame 44000: 213.795563\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 950.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_950-20251108-2118-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_950-20251108-2118-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 953.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_953-20251108-2118-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_953-20251108-2118-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 44500\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 956.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_956-20251108-2119-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_956-20251108-2119-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 961.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_961-20251108-2119-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_961-20251108-2119-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 45000\n",
            "[DEBUG] Loss at frame 45000: 430.063263\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 961.00\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 963.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_963-20251108-2119-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_963-20251108-2119-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 45500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 982.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_982-20251108-2119-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_982-20251108-2119-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 46000\n",
            "[DEBUG] Loss at frame 46000: 295.989807\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 991.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_991-20251108-2119-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_991-20251108-2119-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 46500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1010.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1010-20251108-2119-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1010-20251108-2119-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1026.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1026-20251108-2119-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1026-20251108-2119-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 47000\n",
            "[DEBUG] Loss at frame 47000: 558.913025\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1041.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1041-20251108-2119-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1041-20251108-2119-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 47500\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 1045.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1045-20251108-2119-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1045-20251108-2119-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1045.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 48000\n",
            "[DEBUG] Loss at frame 48000: 376.656128\n",
            "[DEBUG] Episode finished with reward 1600.00, mean_100 1057.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1057-20251108-2119-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1057-20251108-2119-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1066.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1066-20251108-2119-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1066-20251108-2119-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 48500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1083.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1083-20251108-2119-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1083-20251108-2119-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1088.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1088-20251108-2119-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1088-20251108-2119-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 49000\n",
            "[DEBUG] Loss at frame 49000: 226.400497\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1076.00\n",
            "[DEBUG] Target network synced at frame 49500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1090.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1090-20251108-2119-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1090-20251108-2119-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 1103.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1103-20251108-2120-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1103-20251108-2120-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 50000\n",
            "[DEBUG] Loss at frame 50000: 474.184265\n",
            "[DEBUG] Action distribution at 50000: {np.int64(0): np.int64(2), np.int64(1): np.int64(1), np.int64(4): np.int64(4), np.int64(5): np.int64(3), np.int64(11): np.int64(1), np.int64(12): np.int64(1), np.int64(13): np.int64(3), np.int64(16): np.int64(1)}\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1112.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1112-20251108-2120-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1112-20251108-2120-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1114.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1114-20251108-2120-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1114-20251108-2120-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 50500\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1129.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1129-20251108-2120-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1129-20251108-2120-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 1140.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1140-20251108-2120-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1140-20251108-2120-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1128.00\n",
            "[DEBUG] Target network synced at frame 51000\n",
            "[DEBUG] Loss at frame 51000: 308.544128\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1137.00\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1142.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1142-20251108-2120-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1142-20251108-2120-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 51500\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 1151.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1151-20251108-2120-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1151-20251108-2120-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 52000\n",
            "[DEBUG] Loss at frame 52000: 470.963745\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1162.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1162-20251108-2120-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1162-20251108-2120-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 52500\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1164.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1164-20251108-2120-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1164-20251108-2120-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1159.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1145.00\n",
            "[DEBUG] Target network synced at frame 53000\n",
            "[DEBUG] Loss at frame 53000: 588.196289\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1147.00\n",
            "[DEBUG] Target network synced at frame 53500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1164.00\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1169.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1169-20251108-2120-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1169-20251108-2120-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 54000\n",
            "[DEBUG] Loss at frame 54000: 594.600464\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1166.00\n",
            "[DEBUG] Target network synced at frame 54500\n",
            "[DEBUG] Episode finished with reward 900.00, mean_100 1166.00\n",
            "[DEBUG] Episode finished with reward 1800.00, mean_100 1183.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1183-20251108-2121-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1183-20251108-2121-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 55000\n",
            "[DEBUG] Loss at frame 55000: 548.401001\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1194.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1194-20251108-2121-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1194-20251108-2121-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1177.00\n",
            "[DEBUG] Target network synced at frame 55500\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1181.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 56000\n",
            "[DEBUG] Loss at frame 56000: 234.272751\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1199.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1199-20251108-2121-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1199-20251108-2121-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1207.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1207-20251108-2121-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1207-20251108-2121-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 56500\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 1197.00\n",
            "[DEBUG] Target network synced at frame 57000\n",
            "[DEBUG] Loss at frame 57000: 868.835388\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1197.00\n",
            "[DEBUG] Target network synced at frame 57500\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1217.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1217-20251108-2121-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1217-20251108-2121-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1204.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 58000\n",
            "[DEBUG] Loss at frame 58000: 284.298126\n",
            "[DEBUG] Episode finished with reward 900.00, mean_100 1191.00\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1200.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1200.00\n",
            "[DEBUG] Target network synced at frame 58500\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 1210.00\n",
            "[DEBUG] Target network synced at frame 59000\n",
            "[DEBUG] Loss at frame 59000: 583.294861\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1217.00\n",
            "[DEBUG] Target network synced at frame 59500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1232.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1232-20251108-2121-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1232-20251108-2121-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1226.00\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1237.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1237-20251108-2121-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1237-20251108-2121-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 60000\n",
            "[DEBUG] Loss at frame 60000: 228.379486\n",
            "[DEBUG] Action distribution at 60000: {np.int64(0): np.int64(3), np.int64(3): np.int64(1), np.int64(4): np.int64(3), np.int64(5): np.int64(2), np.int64(7): np.int64(1), np.int64(9): np.int64(1), np.int64(10): np.int64(2), np.int64(12): np.int64(1), np.int64(16): np.int64(2)}\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1239.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1239-20251108-2122-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1239-20251108-2122-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 60500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1252.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1252-20251108-2122-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1252-20251108-2122-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 61000\n",
            "[DEBUG] Loss at frame 61000: 228.498871\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1253.00\n",
            "[DEBUG] Target network synced at frame 61500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1267.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1267-20251108-2122-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1267-20251108-2122-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1246.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 62000\n",
            "[DEBUG] Loss at frame 62000: 464.571411\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1246.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1245.00\n",
            "[DEBUG] Target network synced at frame 62500\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1247.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1250.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1241.00\n",
            "[DEBUG] Target network synced at frame 63000\n",
            "[DEBUG] Loss at frame 63000: 377.951477\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1259.00\n",
            "[DEBUG] Target network synced at frame 63500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1277.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1277-20251108-2122-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1277-20251108-2122-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 64000\n",
            "[DEBUG] Loss at frame 64000: 530.042542\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1297.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1297-20251108-2122-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1297-20251108-2122-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 64500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1297.00\n",
            "[DEBUG] Target network synced at frame 65000\n",
            "[DEBUG] Loss at frame 65000: 235.727264\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1315.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1315-20251108-2122-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1315-20251108-2122-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1316.00\n",
            "[DEBUG] Target network synced at frame 65500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1330.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1330-20251108-2123-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1330-20251108-2123-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1316.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 66000\n",
            "[DEBUG] Loss at frame 66000: 530.663086\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1336.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1336-20251108-2123-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1336-20251108-2123-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 66500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1344.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1344-20251108-2123-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1344-20251108-2123-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1362.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1362-20251108-2123-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1362-20251108-2123-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 67000\n",
            "[DEBUG] Loss at frame 67000: 281.661560\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1358.00\n",
            "[DEBUG] Target network synced at frame 67500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1359.00\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1342.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 68000\n",
            "[DEBUG] Loss at frame 68000: 297.194519\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1348.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1337.00\n",
            "[DEBUG] Target network synced at frame 68500\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 1331.00\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1351.00\n",
            "[DEBUG] Target network synced at frame 69000\n",
            "[DEBUG] Loss at frame 69000: 447.698151\n",
            "[DEBUG] Episode finished with reward 1800.00, mean_100 1355.00\n",
            "[DEBUG] Target network synced at frame 69500\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 1347.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1328.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1312.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 70000\n",
            "[DEBUG] Loss at frame 70000: 773.080933\n",
            "[DEBUG] Action distribution at 70000: {np.int64(0): np.int64(3), np.int64(3): np.int64(1), np.int64(4): np.int64(2), np.int64(7): np.int64(1), np.int64(9): np.int64(1), np.int64(10): np.int64(2), np.int64(11): np.int64(1), np.int64(12): np.int64(3), np.int64(14): np.int64(1), np.int64(16): np.int64(1)}\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 1296.00\n",
            "[DEBUG] Target network synced at frame 70500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1309.00\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1308.00\n",
            "[DEBUG] Target network synced at frame 71000\n",
            "[DEBUG] Loss at frame 71000: 442.782532\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1327.00\n",
            "[DEBUG] Target network synced at frame 71500\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1324.00\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1335.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 72000\n",
            "[DEBUG] Loss at frame 72000: 166.285004\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 1325.00\n",
            "[DEBUG] Target network synced at frame 72500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1336.00\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1329.00\n",
            "[DEBUG] Target network synced at frame 73000\n",
            "[DEBUG] Loss at frame 73000: 151.113708\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 1338.00\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1330.00\n",
            "[DEBUG] Target network synced at frame 73500\n",
            "[DEBUG] Episode finished with reward 1800.00, mean_100 1334.00\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1322.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 74000\n",
            "[DEBUG] Loss at frame 74000: 281.832642\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1323.00\n",
            "[DEBUG] Target network synced at frame 74500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1338.00\n",
            "[DEBUG] Target network synced at frame 75000\n",
            "[DEBUG] Loss at frame 75000: 135.241028\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1355.00\n",
            "[DEBUG] Target network synced at frame 75500\n",
            "[DEBUG] Episode finished with reward 1600.00, mean_100 1359.00\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1374.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1374-20251108-2125-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1374-20251108-2125-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 76000\n",
            "[DEBUG] Loss at frame 76000: 296.503784\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1388.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1388-20251108-2125-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1388-20251108-2125-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 76500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1388.00\n",
            "[DEBUG] Target network synced at frame 77000\n",
            "[DEBUG] Loss at frame 77000: 484.234314\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1375.00\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1363.00\n",
            "[DEBUG] Target network synced at frame 77500\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 1357.00\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1341.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 78000\n",
            "[DEBUG] Loss at frame 78000: 389.071014\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1348.00\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 1359.00\n",
            "[DEBUG] Target network synced at frame 78500\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 1349.00\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1358.00\n",
            "[DEBUG] Target network synced at frame 79000\n",
            "[DEBUG] Loss at frame 79000: 635.022461\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 1343.00\n",
            "[DEBUG] Target network synced at frame 79500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1358.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1357.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1338.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 80000\n",
            "[DEBUG] Loss at frame 80000: 265.260132\n",
            "[DEBUG] Action distribution at 80000: {np.int64(0): np.int64(1), np.int64(1): np.int64(1), np.int64(2): np.int64(1), np.int64(4): np.int64(3), np.int64(7): np.int64(4), np.int64(9): np.int64(2), np.int64(10): np.int64(2), np.int64(14): np.int64(1), np.int64(16): np.int64(1)}\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1330.00\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1340.00\n",
            "[DEBUG] Target network synced at frame 80500\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1348.00\n",
            "[DEBUG] Target network synced at frame 81000\n",
            "[DEBUG] Loss at frame 81000: 254.741104\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1351.00\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1347.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1347.00\n",
            "[DEBUG] Target network synced at frame 81500\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1339.00\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1339.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1329.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 82000\n",
            "[DEBUG] Loss at frame 82000: 659.165039\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1330.00\n",
            "[DEBUG] Target network synced at frame 82500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1329.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1329.00\n",
            "[DEBUG] Target network synced at frame 83000\n",
            "[DEBUG] Loss at frame 83000: 450.491577\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1332.00\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1332.00\n",
            "[DEBUG] Target network synced at frame 83500\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1316.00\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1315.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 84000\n",
            "[DEBUG] Loss at frame 84000: 410.290924\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1313.00\n",
            "[DEBUG] Target network synced at frame 84500\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1317.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1300.00\n",
            "[DEBUG] Target network synced at frame 85000\n",
            "[DEBUG] Loss at frame 85000: 397.032623\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1298.00\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1320.00\n",
            "[DEBUG] Target network synced at frame 85500\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 1316.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 86000\n",
            "[DEBUG] Loss at frame 86000: 275.510590\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1319.00\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1307.00\n",
            "[DEBUG] Target network synced at frame 86500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1316.00\n",
            "[DEBUG] Target network synced at frame 87000\n",
            "[DEBUG] Loss at frame 87000: 155.698944\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1306.00\n",
            "[DEBUG] Target network synced at frame 87500\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1306.00\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1304.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 88000\n",
            "[DEBUG] Loss at frame 88000: 260.229553\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1316.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1306.00\n",
            "[DEBUG] Target network synced at frame 88500\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1321.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1307.00\n",
            "[DEBUG] Target network synced at frame 89000\n",
            "[DEBUG] Loss at frame 89000: 112.442131\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1311.00\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1312.00\n",
            "[DEBUG] Target network synced at frame 89500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1329.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 90000\n",
            "[DEBUG] Loss at frame 90000: 354.192200\n",
            "[DEBUG] Action distribution at 90000: {np.int64(0): np.int64(2), np.int64(2): np.int64(1), np.int64(4): np.int64(2), np.int64(7): np.int64(1), np.int64(9): np.int64(3), np.int64(13): np.int64(1), np.int64(15): np.int64(2), np.int64(16): np.int64(4)}\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1335.00\n",
            "[DEBUG] Target network synced at frame 90500\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1316.00\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1315.00\n",
            "[DEBUG] Target network synced at frame 91000\n",
            "[DEBUG] Loss at frame 91000: 279.068787\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1295.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1276.00\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1283.00\n",
            "[DEBUG] Target network synced at frame 91500\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1274.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 92000\n",
            "[DEBUG] Loss at frame 92000: 175.750458\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1294.00\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1297.00\n",
            "[DEBUG] Target network synced at frame 92500\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 1307.00\n",
            "[DEBUG] Target network synced at frame 93000\n",
            "[DEBUG] Loss at frame 93000: 175.840622\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1325.00\n",
            "[DEBUG] Target network synced at frame 93500\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1325.00\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1310.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1288.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 94000\n",
            "[DEBUG] Loss at frame 94000: 213.738800\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1287.00\n",
            "[DEBUG] Target network synced at frame 94500\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1271.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1272.00\n",
            "[DEBUG] Target network synced at frame 95000\n",
            "[DEBUG] Loss at frame 95000: 127.846672\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1272.00\n",
            "[DEBUG] Target network synced at frame 95500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1291.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1271.00\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 1263.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 96000\n",
            "[DEBUG] Loss at frame 96000: 416.783875\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1248.00\n",
            "[DEBUG] Target network synced at frame 96500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1255.00\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 1240.00\n",
            "[DEBUG] Target network synced at frame 97000\n",
            "[DEBUG] Loss at frame 97000: 116.644089\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1254.00\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1243.00\n",
            "[DEBUG] Target network synced at frame 97500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1263.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 98000\n",
            "[DEBUG] Loss at frame 98000: 77.378181\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1278.00\n",
            "[DEBUG] Target network synced at frame 98500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1278.00\n",
            "[DEBUG] Target network synced at frame 99000\n",
            "[DEBUG] Loss at frame 99000: 702.593140\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1280.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1270.00\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1269.00\n",
            "[DEBUG] Target network synced at frame 99500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1285.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 100000\n",
            "[DEBUG] Loss at frame 100000: 222.922485\n",
            "[DEBUG] Action distribution at 100000: {np.int64(4): np.int64(2), np.int64(5): np.int64(1), np.int64(6): np.int64(1), np.int64(7): np.int64(1), np.int64(9): np.int64(1), np.int64(10): np.int64(2), np.int64(12): np.int64(1), np.int64(13): np.int64(1), np.int64(14): np.int64(4), np.int64(16): np.int64(1), np.int64(17): np.int64(1)}\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1289.00\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1287.00\n",
            "[DEBUG] Target network synced at frame 100500\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1269.00\n",
            "[DEBUG] Target network synced at frame 101000\n",
            "[DEBUG] Loss at frame 101000: 73.588875\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1270.00\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1278.00\n",
            "[DEBUG] Target network synced at frame 101500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1284.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 102000\n",
            "[DEBUG] Loss at frame 102000: 133.407074\n",
            "[DEBUG] Episode finished with reward 1600.00, mean_100 1288.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1272.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1265.00\n",
            "[DEBUG] Target network synced at frame 102500\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 1259.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1248.00\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1235.00\n",
            "[DEBUG] Target network synced at frame 103000\n",
            "[DEBUG] Loss at frame 103000: 210.773254\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1242.00\n",
            "[DEBUG] Target network synced at frame 103500\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 1235.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 104000\n",
            "[DEBUG] Loss at frame 104000: 235.739197\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1234.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1214.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1202.00\n",
            "[DEBUG] Target network synced at frame 104500\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 1196.00\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1183.00\n",
            "[DEBUG] Target network synced at frame 105000\n",
            "[DEBUG] Loss at frame 105000: 431.234192\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1182.00\n",
            "[DEBUG] Target network synced at frame 105500\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1192.00\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1191.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 106000\n",
            "[DEBUG] Loss at frame 106000: 240.157303\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1198.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1193.00\n",
            "[DEBUG] Target network synced at frame 106500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1180.00\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1172.00\n",
            "[DEBUG] Target network synced at frame 107000\n",
            "[DEBUG] Loss at frame 107000: 318.975220\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1186.00\n",
            "[DEBUG] Target network synced at frame 107500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1185.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 108000\n",
            "[DEBUG] Loss at frame 108000: 861.001221\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1198.00\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1197.00\n",
            "[DEBUG] Target network synced at frame 108500\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1193.00\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1199.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1196.00\n",
            "[DEBUG] Target network synced at frame 109000\n",
            "[DEBUG] Loss at frame 109000: 616.622925\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1189.00\n",
            "[DEBUG] Target network synced at frame 109500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1197.00\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1184.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 110000\n",
            "[DEBUG] Loss at frame 110000: 324.877228\n",
            "[DEBUG] Action distribution at 110000: {np.int64(0): np.int64(1), np.int64(4): np.int64(2), np.int64(6): np.int64(1), np.int64(7): np.int64(1), np.int64(8): np.int64(1), np.int64(9): np.int64(3), np.int64(10): np.int64(1), np.int64(12): np.int64(2), np.int64(14): np.int64(2), np.int64(15): np.int64(1), np.int64(17): np.int64(1)}\n",
            "[DEBUG] Episode finished with reward 900.00, mean_100 1185.00\n",
            "[DEBUG] Target network synced at frame 110500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1202.00\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1212.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1206.00\n",
            "[DEBUG] Target network synced at frame 111000\n",
            "[DEBUG] Loss at frame 111000: 417.569397\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1224.00\n",
            "[DEBUG] Target network synced at frame 111500\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1214.00\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1201.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 112000\n",
            "[DEBUG] Loss at frame 112000: 147.326050\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1211.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1206.00\n",
            "[DEBUG] Target network synced at frame 112500\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1197.00\n",
            "[DEBUG] Target network synced at frame 113000\n",
            "[DEBUG] Loss at frame 113000: 163.887314\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1213.00\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1213.00\n",
            "[DEBUG] Target network synced at frame 113500\n",
            "[DEBUG] Episode finished with reward 900.00, mean_100 1211.00\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1201.00\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1203.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 114000\n",
            "[DEBUG] Loss at frame 114000: 294.495239\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 1196.00\n",
            "[DEBUG] Target network synced at frame 114500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1195.00\n",
            "[DEBUG] Episode finished with reward 900.00, mean_100 1198.00\n",
            "[DEBUG] Target network synced at frame 115000\n",
            "[DEBUG] Loss at frame 115000: 411.610626\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1186.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1182.00\n",
            "[DEBUG] Target network synced at frame 115500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1182.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 116000\n",
            "[DEBUG] Loss at frame 116000: 659.591919\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 1187.00\n",
            "[DEBUG] Episode finished with reward 1600.00, mean_100 1181.00\n",
            "[DEBUG] Target network synced at frame 116500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1196.00\n",
            "[DEBUG] Target network synced at frame 117000\n",
            "[DEBUG] Loss at frame 117000: 502.500031\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1186.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1187.00\n",
            "[DEBUG] Target network synced at frame 117500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1191.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 118000\n",
            "[DEBUG] Loss at frame 118000: 608.668396\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1211.00\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 1205.00\n",
            "[DEBUG] Target network synced at frame 118500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1204.00\n",
            "[DEBUG] Target network synced at frame 119000\n",
            "[DEBUG] Loss at frame 119000: 730.943115\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1204.00\n",
            "[DEBUG] Target network synced at frame 119500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1205.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1206.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 120000\n",
            "[DEBUG] Loss at frame 120000: 159.438873\n",
            "[DEBUG] Action distribution at 120000: {np.int64(1): np.int64(1), np.int64(2): np.int64(3), np.int64(4): np.int64(3), np.int64(10): np.int64(2), np.int64(12): np.int64(4), np.int64(13): np.int64(1), np.int64(14): np.int64(1), np.int64(15): np.int64(1)}\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1196.00\n",
            "[DEBUG] Target network synced at frame 120500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1216.00\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1234.00\n",
            "[DEBUG] Target network synced at frame 121000\n",
            "[DEBUG] Loss at frame 121000: 207.710815\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1227.00\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1237.00\n",
            "[DEBUG] Target network synced at frame 121500\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 1230.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 122000\n",
            "[DEBUG] Loss at frame 122000: 223.753082\n",
            "[DEBUG] Episode finished with reward 900.00, mean_100 1219.00\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 1217.00\n",
            "[DEBUG] Target network synced at frame 122500\n",
            "[DEBUG] Episode finished with reward 1600.00, mean_100 1213.00\n",
            "[DEBUG] Target network synced at frame 123000\n",
            "[DEBUG] Loss at frame 123000: 353.101654\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1212.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1211.00\n",
            "[DEBUG] Target network synced at frame 123500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1232.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 124000\n",
            "[DEBUG] Loss at frame 124000: 584.634277\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1234.00\n",
            "[DEBUG] Episode finished with reward 900.00, mean_100 1238.00\n",
            "[DEBUG] Target network synced at frame 124500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1254.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1237.00\n",
            "[DEBUG] Target network synced at frame 125000\n",
            "[DEBUG] Loss at frame 125000: 513.352600\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1237.00\n",
            "[DEBUG] Target network synced at frame 125500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1256.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 126000\n",
            "[DEBUG] Loss at frame 126000: 310.952637\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1255.00\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 1263.00\n",
            "[DEBUG] Target network synced at frame 126500\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1247.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1241.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1228.00\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1223.00\n",
            "[DEBUG] Target network synced at frame 127000\n",
            "[DEBUG] Loss at frame 127000: 572.387451\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1205.00\n",
            "[DEBUG] Target network synced at frame 127500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1205.00\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1196.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 128000\n",
            "[DEBUG] Loss at frame 128000: 386.605743\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1197.00\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1205.00\n",
            "[DEBUG] Target network synced at frame 128500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1223.00\n",
            "[DEBUG] Target network synced at frame 129000\n",
            "[DEBUG] Loss at frame 129000: 282.525879\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1223.00\n",
            "[DEBUG] Target network synced at frame 129500\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 1228.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1213.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 130000\n",
            "[DEBUG] Loss at frame 130000: 329.896729\n",
            "[DEBUG] Action distribution at 130000: {np.int64(1): np.int64(2), np.int64(2): np.int64(1), np.int64(4): np.int64(3), np.int64(9): np.int64(5), np.int64(12): np.int64(1), np.int64(13): np.int64(1), np.int64(15): np.int64(1), np.int64(16): np.int64(1), np.int64(17): np.int64(1)}\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1231.00\n",
            "[DEBUG] Target network synced at frame 130500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1230.00\n",
            "[DEBUG] Target network synced at frame 131000\n",
            "[DEBUG] Loss at frame 131000: 170.814301\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1237.00\n",
            "[DEBUG] Episode finished with reward 900.00, mean_100 1227.00\n",
            "[DEBUG] Target network synced at frame 131500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1232.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 132000\n",
            "[DEBUG] Loss at frame 132000: 153.430496\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 1243.00\n",
            "[DEBUG] Target network synced at frame 132500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1264.00\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1278.00\n",
            "[DEBUG] Target network synced at frame 133000\n",
            "[DEBUG] Loss at frame 133000: 230.314728\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1295.00\n",
            "[DEBUG] Target network synced at frame 133500\n",
            "[DEBUG] Episode finished with reward 1600.00, mean_100 1306.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1290.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 134000\n",
            "[DEBUG] Loss at frame 134000: 216.863190\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 1282.00\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1275.00\n",
            "[DEBUG] Target network synced at frame 134500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1294.00\n",
            "[DEBUG] Target network synced at frame 135000\n",
            "[DEBUG] Loss at frame 135000: 129.497177\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1307.00\n",
            "[DEBUG] Target network synced at frame 135500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1312.00\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1307.00\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1299.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 136000\n",
            "[DEBUG] Loss at frame 136000: 172.520996\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1284.00\n",
            "[DEBUG] Target network synced at frame 136500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1297.00\n",
            "[DEBUG] Target network synced at frame 137000\n",
            "[DEBUG] Loss at frame 137000: 141.769211\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1296.00\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 1302.00\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1307.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1302.00\n",
            "[DEBUG] Target network synced at frame 137500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1303.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 138000\n",
            "[DEBUG] Loss at frame 138000: 276.942841\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1304.00\n",
            "[DEBUG] Target network synced at frame 138500\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1307.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1289.00\n",
            "[DEBUG] Target network synced at frame 139000\n",
            "[DEBUG] Loss at frame 139000: 178.588745\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 1303.00\n",
            "[DEBUG] Target network synced at frame 139500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1316.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 140000\n",
            "[DEBUG] Loss at frame 140000: 503.170959\n",
            "[DEBUG] Action distribution at 140000: {np.int64(0): np.int64(1), np.int64(4): np.int64(7), np.int64(5): np.int64(2), np.int64(9): np.int64(1), np.int64(12): np.int64(1), np.int64(15): np.int64(2), np.int64(17): np.int64(2)}\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1334.00\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1342.00\n",
            "[DEBUG] Target network synced at frame 140500\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1345.00\n",
            "[DEBUG] Target network synced at frame 141000\n",
            "[DEBUG] Loss at frame 141000: 148.333466\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 1353.00\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1349.00\n",
            "[DEBUG] Target network synced at frame 141500\n",
            "[DEBUG] Episode finished with reward 900.00, mean_100 1339.00\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1336.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 142000\n",
            "[DEBUG] Loss at frame 142000: 135.067291\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 1348.00\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1335.00\n",
            "[DEBUG] Target network synced at frame 142500\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 1339.00\n",
            "[DEBUG] Target network synced at frame 143000\n",
            "[DEBUG] Loss at frame 143000: 352.830017\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1352.00\n",
            "[DEBUG] Target network synced at frame 143500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1362.00\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1379.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 144000\n",
            "[DEBUG] Loss at frame 144000: 255.037308\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1388.00\n",
            "[DEBUG] Target network synced at frame 144500\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1376.00\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1364.00\n",
            "[DEBUG] Target network synced at frame 145000\n",
            "[DEBUG] Loss at frame 145000: 111.523445\n",
            "[DEBUG] Episode finished with reward 900.00, mean_100 1364.00\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1378.00\n",
            "[DEBUG] Target network synced at frame 145500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1396.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1396-20251108-2138-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1396-20251108-2138-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 146000\n",
            "[DEBUG] Loss at frame 146000: 872.540466\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1401.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1401-20251108-2138-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1401-20251108-2138-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 146500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1400.00\n",
            "[DEBUG] Target network synced at frame 147000\n",
            "[DEBUG] Loss at frame 147000: 282.834595\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1410.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1410-20251108-2138-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1410-20251108-2138-variant_double_dueling_per.dat\n",
            "[DEBUG] Target network synced at frame 147500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1421.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1421-20251108-2138-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1421-20251108-2138-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1436.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1436-20251108-2138-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1436-20251108-2138-variant_double_dueling_per.dat\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 148000\n",
            "[DEBUG] Loss at frame 148000: 250.089310\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1435.00\n",
            "[DEBUG] Target network synced at frame 148500\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 1426.00\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1415.00\n",
            "[DEBUG] Target network synced at frame 149000\n",
            "[DEBUG] Loss at frame 149000: 236.401184\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1414.00\n",
            "[DEBUG] Target network synced at frame 149500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1422.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 150000\n",
            "[DEBUG] Loss at frame 150000: 223.009796\n",
            "[DEBUG] Action distribution at 150000: {np.int64(0): np.int64(1), np.int64(4): np.int64(5), np.int64(5): np.int64(1), np.int64(7): np.int64(1), np.int64(9): np.int64(1), np.int64(14): np.int64(2), np.int64(16): np.int64(2), np.int64(17): np.int64(3)}\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1439.00\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_DonkeyKong-v5-best_1439-20251108-2138-variant_double_dueling_per.dat\n",
            " - Local: saved_models/ALE_DonkeyKong-v5-best_1439-20251108-2138-variant_double_dueling_per.dat\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1420.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1401.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1390.00\n",
            "[DEBUG] Target network synced at frame 150500\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1372.00\n",
            "[DEBUG] Target network synced at frame 151000\n",
            "[DEBUG] Loss at frame 151000: 1387.701172\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1372.00\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 1366.00\n",
            "[DEBUG] Target network synced at frame 151500\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 1376.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 152000\n",
            "[DEBUG] Loss at frame 152000: 166.589569\n",
            "[DEBUG] Episode finished with reward 1800.00, mean_100 1384.00\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1380.00\n",
            "[DEBUG] Target network synced at frame 152500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1380.00\n",
            "[DEBUG] Target network synced at frame 153000\n",
            "[DEBUG] Loss at frame 153000: 307.105225\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1387.00\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1379.00\n",
            "[DEBUG] Target network synced at frame 153500\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1382.00\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1376.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 154000\n",
            "[DEBUG] Loss at frame 154000: 80.400459\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1384.00\n",
            "[DEBUG] Target network synced at frame 154500\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1379.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1360.00\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1361.00\n",
            "[DEBUG] Target network synced at frame 155000\n",
            "[DEBUG] Loss at frame 155000: 109.888519\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1359.00\n",
            "[DEBUG] Target network synced at frame 155500\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1341.00\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 1344.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 156000\n",
            "[DEBUG] Loss at frame 156000: 741.264771\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1343.00\n",
            "[DEBUG] Target network synced at frame 156500\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 1351.00\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1350.00\n",
            "[DEBUG] Target network synced at frame 157000\n",
            "[DEBUG] Loss at frame 157000: 1001.583374\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1338.00\n",
            "[DEBUG] Target network synced at frame 157500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1346.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 158000\n",
            "[DEBUG] Loss at frame 158000: 634.275391\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1352.00\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1367.00\n",
            "[DEBUG] Target network synced at frame 158500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1386.00\n",
            "[DEBUG] Target network synced at frame 159000\n",
            "[DEBUG] Loss at frame 159000: 141.605286\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1399.00\n",
            "[DEBUG] Target network synced at frame 159500\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1402.00\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1403.00\n",
            "[DEBUG] Episode finished with reward 1400.00, mean_100 1396.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 160000\n",
            "[DEBUG] Loss at frame 160000: 171.720642\n",
            "[DEBUG] Action distribution at 160000: {np.int64(2): np.int64(1), np.int64(4): np.int64(6), np.int64(7): np.int64(2), np.int64(11): np.int64(1), np.int64(14): np.int64(3), np.int64(16): np.int64(1), np.int64(17): np.int64(2)}\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1385.00\n",
            "[DEBUG] Target network synced at frame 160500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1384.00\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1393.00\n",
            "[DEBUG] Target network synced at frame 161000\n",
            "[DEBUG] Loss at frame 161000: 483.633057\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1373.00\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 1359.00\n",
            "[DEBUG] Target network synced at frame 161500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1364.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 162000\n",
            "[DEBUG] Loss at frame 162000: 116.454239\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1379.00\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1368.00\n",
            "[DEBUG] Target network synced at frame 162500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1367.00\n",
            "[DEBUG] Target network synced at frame 163000\n",
            "[DEBUG] Loss at frame 163000: 262.928497\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1367.00\n",
            "[DEBUG] Target network synced at frame 163500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1377.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1360.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1349.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 164000\n",
            "[DEBUG] Loss at frame 164000: 320.422791\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1347.00\n",
            "[DEBUG] Target network synced at frame 164500\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1331.00\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1331.00\n",
            "[DEBUG] Target network synced at frame 165000\n",
            "[DEBUG] Loss at frame 165000: 197.188278\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1323.00\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1325.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1323.00\n",
            "[DEBUG] Target network synced at frame 165500\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1320.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 166000\n",
            "[DEBUG] Loss at frame 166000: 235.970444\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1319.00\n",
            "[DEBUG] Target network synced at frame 166500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1322.00\n",
            "[DEBUG] Target network synced at frame 167000\n",
            "[DEBUG] Loss at frame 167000: 833.462158\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1322.00\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1338.00\n",
            "[DEBUG] Target network synced at frame 167500\n",
            "[DEBUG] Episode finished with reward 900.00, mean_100 1336.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 168000\n",
            "[DEBUG] Loss at frame 168000: 358.517365\n",
            "[DEBUG] Episode finished with reward 1600.00, mean_100 1350.00\n",
            "[DEBUG] Target network synced at frame 168500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1349.00\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1346.00\n",
            "[DEBUG] Target network synced at frame 169000\n",
            "[DEBUG] Loss at frame 169000: 112.580399\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1345.00\n",
            "[DEBUG] Target network synced at frame 169500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1359.00\n",
            "[DEBUG] Episode finished with reward 1800.00, mean_100 1375.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 170000\n",
            "[DEBUG] Loss at frame 170000: 293.036316\n",
            "[DEBUG] Action distribution at 170000: {np.int64(3): np.int64(1), np.int64(6): np.int64(1), np.int64(8): np.int64(1), np.int64(9): np.int64(1), np.int64(10): np.int64(1), np.int64(11): np.int64(2), np.int64(12): np.int64(5), np.int64(14): np.int64(1), np.int64(15): np.int64(1), np.int64(16): np.int64(2)}\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1374.00\n",
            "[DEBUG] Target network synced at frame 170500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1373.00\n",
            "[DEBUG] Target network synced at frame 171000\n",
            "[DEBUG] Loss at frame 171000: 208.191910\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1371.00\n",
            "[DEBUG] Target network synced at frame 171500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1390.00\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1386.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 172000\n",
            "[DEBUG] Loss at frame 172000: 142.518768\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1386.00\n",
            "[DEBUG] Target network synced at frame 172500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1385.00\n",
            "[DEBUG] Target network synced at frame 173000\n",
            "[DEBUG] Loss at frame 173000: 119.930573\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1383.00\n",
            "[DEBUG] Target network synced at frame 173500\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1383.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 174000\n",
            "[DEBUG] Loss at frame 174000: 306.739807\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1387.00\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1392.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1383.00\n",
            "[DEBUG] Target network synced at frame 174500\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1378.00\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1384.00\n",
            "[DEBUG] Target network synced at frame 175000\n",
            "[DEBUG] Loss at frame 175000: 194.110229\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1384.00\n",
            "[DEBUG] Target network synced at frame 175500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1389.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1368.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 176000\n",
            "[DEBUG] Loss at frame 176000: 90.206833\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1367.00\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1355.00\n",
            "[DEBUG] Target network synced at frame 176500\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 1348.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1344.00\n",
            "[DEBUG] Target network synced at frame 177000\n",
            "[DEBUG] Loss at frame 177000: 271.658508\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1356.00\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1350.00\n",
            "[DEBUG] Target network synced at frame 177500\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 1348.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1329.00\n",
            "[DEBUG] Episode finished with reward 600.00, mean_100 1318.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 178000\n",
            "[DEBUG] Loss at frame 178000: 366.224060\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1300.00\n",
            "[DEBUG] Episode finished with reward 300.00, mean_100 1284.00\n",
            "[DEBUG] Target network synced at frame 178500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1284.00\n",
            "[DEBUG] Target network synced at frame 179000\n",
            "[DEBUG] Loss at frame 179000: 499.068848\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1284.00\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1284.00\n",
            "[DEBUG] Target network synced at frame 179500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1297.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 180000\n",
            "[DEBUG] Loss at frame 180000: 180.590088\n",
            "[DEBUG] Action distribution at 180000: {np.int64(0): np.int64(1), np.int64(3): np.int64(1), np.int64(4): np.int64(4), np.int64(6): np.int64(1), np.int64(7): np.int64(2), np.int64(8): np.int64(1), np.int64(9): np.int64(1), np.int64(12): np.int64(1), np.int64(14): np.int64(2), np.int64(16): np.int64(1), np.int64(17): np.int64(1)}\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1312.00\n",
            "[DEBUG] Target network synced at frame 180500\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1304.00\n",
            "[DEBUG] Target network synced at frame 181000\n",
            "[DEBUG] Loss at frame 181000: 176.979156\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1306.00\n",
            "[DEBUG] Episode finished with reward 1600.00, mean_100 1303.00\n",
            "[DEBUG] Target network synced at frame 181500\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1305.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1304.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 182000\n",
            "[DEBUG] Loss at frame 182000: 628.123291\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1317.00\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1334.00\n",
            "[DEBUG] Target network synced at frame 182500\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1335.00\n",
            "[DEBUG] Target network synced at frame 183000\n",
            "[DEBUG] Loss at frame 183000: 345.239380\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1340.00\n",
            "[DEBUG] Target network synced at frame 183500\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1333.00\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1320.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 184000\n",
            "[DEBUG] Loss at frame 184000: 241.670578\n",
            "[DEBUG] Episode finished with reward 1600.00, mean_100 1319.00\n",
            "[DEBUG] Target network synced at frame 184500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1319.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1313.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1301.00\n",
            "[DEBUG] Target network synced at frame 185000\n",
            "[DEBUG] Loss at frame 185000: 157.589447\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1304.00\n",
            "[DEBUG] Target network synced at frame 185500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1320.00\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1307.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 186000\n",
            "[DEBUG] Loss at frame 186000: 560.644592\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1304.00\n",
            "[DEBUG] Target network synced at frame 186500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1323.00\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 1333.00\n",
            "[DEBUG] Target network synced at frame 187000\n",
            "[DEBUG] Loss at frame 187000: 217.827301\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 1329.00\n",
            "[DEBUG] Target network synced at frame 187500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1345.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 188000\n",
            "[DEBUG] Loss at frame 188000: 230.988220\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1353.00\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1347.00\n",
            "[DEBUG] Target network synced at frame 188500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1356.00\n",
            "[DEBUG] Target network synced at frame 189000\n",
            "[DEBUG] Loss at frame 189000: 119.504684\n",
            "[DEBUG] Episode finished with reward 1800.00, mean_100 1355.00\n",
            "[DEBUG] Target network synced at frame 189500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1366.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1347.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 190000\n",
            "[DEBUG] Loss at frame 190000: 246.483994\n",
            "[DEBUG] Action distribution at 190000: {np.int64(0): np.int64(1), np.int64(1): np.int64(1), np.int64(2): np.int64(1), np.int64(4): np.int64(2), np.int64(7): np.int64(1), np.int64(8): np.int64(1), np.int64(9): np.int64(3), np.int64(10): np.int64(1), np.int64(12): np.int64(1), np.int64(14): np.int64(1), np.int64(17): np.int64(3)}\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1343.00\n",
            "[DEBUG] Target network synced at frame 190500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1344.00\n",
            "[DEBUG] Episode finished with reward 1200.00, mean_100 1337.00\n",
            "[DEBUG] Target network synced at frame 191000\n",
            "[DEBUG] Loss at frame 191000: 267.292053\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1340.00\n",
            "[DEBUG] Target network synced at frame 191500\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 1347.00\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1354.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 192000\n",
            "[DEBUG] Loss at frame 192000: 137.711288\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1347.00\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1360.00\n",
            "[DEBUG] Target network synced at frame 192500\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1362.00\n",
            "[DEBUG] Target network synced at frame 193000\n",
            "[DEBUG] Loss at frame 193000: 291.954437\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1347.00\n",
            "[DEBUG] Episode finished with reward 1700.00, mean_100 1363.00\n",
            "[DEBUG] Target network synced at frame 193500\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1362.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 194000\n",
            "[DEBUG] Loss at frame 194000: 254.898544\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1362.00\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1362.00\n",
            "[DEBUG] Target network synced at frame 194500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1372.00\n",
            "[DEBUG] Target network synced at frame 195000\n",
            "[DEBUG] Loss at frame 195000: 167.472046\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1372.00\n",
            "[DEBUG] Target network synced at frame 195500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1373.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1354.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 196000\n",
            "[DEBUG] Loss at frame 196000: 186.467896\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1370.00\n",
            "[DEBUG] Episode finished with reward 700.00, mean_100 1373.00\n",
            "[DEBUG] Target network synced at frame 196500\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1364.00\n",
            "[DEBUG] Target network synced at frame 197000\n",
            "[DEBUG] Loss at frame 197000: 280.680481\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1380.00\n",
            "[DEBUG] Episode finished with reward 100.00, mean_100 1362.00\n",
            "[DEBUG] Episode finished with reward 0.00, mean_100 1354.00\n",
            "[DEBUG] Target network synced at frame 197500\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1362.00\n",
            "[DEBUG] Episode finished with reward 800.00, mean_100 1366.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 198000\n",
            "[DEBUG] Loss at frame 198000: 285.995087\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1375.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1358.00\n",
            "[DEBUG] Target network synced at frame 198500\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1357.00\n",
            "[DEBUG] Target network synced at frame 199000\n",
            "[DEBUG] Loss at frame 199000: 210.906616\n",
            "[DEBUG] Episode finished with reward 1500.00, mean_100 1353.00\n",
            "[DEBUG] Target network synced at frame 199500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1355.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1348.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1334.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 200000\n",
            "[DEBUG] Loss at frame 200000: 141.107208\n",
            "[DEBUG] Action distribution at 200000: {np.int64(1): np.int64(1), np.int64(4): np.int64(1), np.int64(5): np.int64(1), np.int64(7): np.int64(2), np.int64(9): np.int64(1), np.int64(10): np.int64(1), np.int64(11): np.int64(1), np.int64(15): np.int64(7), np.int64(16): np.int64(1)}\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1335.00\n",
            "[DEBUG] Target network synced at frame 200500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1339.00\n",
            "[DEBUG] Target network synced at frame 201000\n",
            "[DEBUG] Loss at frame 201000: 328.009460\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1354.00\n",
            "[DEBUG] Episode finished with reward 400.00, mean_100 1339.00\n",
            "[DEBUG] Target network synced at frame 201500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1341.00\n",
            "[DEBUG] Episode finished with reward 200.00, mean_100 1323.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 202000\n",
            "[DEBUG] Loss at frame 202000: 339.072144\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1324.00\n",
            "[DEBUG] Target network synced at frame 202500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1324.00\n",
            "[DEBUG] Episode finished with reward 1000.00, mean_100 1314.00\n",
            "[DEBUG] Target network synced at frame 203000\n",
            "[DEBUG] Loss at frame 203000: 48.363987\n",
            "[DEBUG] Episode finished with reward 2200.00, mean_100 1326.00\n",
            "[DEBUG] Target network synced at frame 203500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1326.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 204000\n",
            "[DEBUG] Loss at frame 204000: 177.088135\n",
            "[DEBUG] Episode finished with reward 1300.00, mean_100 1318.00\n",
            "[DEBUG] Episode finished with reward 500.00, mean_100 1304.00\n",
            "[DEBUG] Target network synced at frame 204500\n",
            "[DEBUG] Episode finished with reward 2100.00, mean_100 1303.00\n",
            "[DEBUG] Target network synced at frame 205000\n",
            "[DEBUG] Loss at frame 205000: 271.619446\n",
            "[DEBUG] Episode finished with reward 1900.00, mean_100 1303.00\n",
            "[DEBUG] Episode finished with reward 1100.00, mean_100 1304.00\n",
            "[DEBUG] Target network synced at frame 205500\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1324.00\n",
            "[DEBUG] Buffer size: 5000 / 1000\n",
            "[DEBUG] Target network synced at frame 206000\n",
            "[DEBUG] Loss at frame 206000: 113.643509\n",
            "[DEBUG] Episode finished with reward 2000.00, mean_100 1339.00\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2028564625.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPSILON_FINAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON_START\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mEPSILON_DECAY_LAST_FRAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mtotal_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1220444818.py\u001b[0m in \u001b[0;36mplay_step\u001b[0;34m(self, net, device, epsilon)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Step in environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;34m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mObsType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrapperObsType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-712661453.py\u001b[0m in \u001b[0;36mobservation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}